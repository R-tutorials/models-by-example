[["index.html", "Model Estimation by Example Demonstrations with R and friends.", " Model Estimation by Example Demonstrations with R and friends. Michael Clark m-clark.github.io 2020-11-01 "],["introduction.html", "Introduction", " Introduction This document provides ‘by-hand’ demonstrations of various models and algorithms. The goal is to take away some of the mystery by providing clean code examples that are easy to run and compare with other tools. The code is not meant to be extensive, or used in production. Almost everything here has a package/module that would do it far better and efficiently. The document itself is also not an introduction to any of these methods, and in fact contains very little expository text. This document is merely a learning tool for those wanting to dive a little deeper. The source code for these demonstrations may be found at their original home here: https://github.com/m-clark/Miscellaneous-R-Code While I’m happy to fix any glaring errors, this is pretty much a completed document, except on the off chance I add to it. This code has accumulated over years, and I just wanted it in a nicer format. Perhaps if others would like to add to it via pull requests, I would do so. "],["linear-regression.html", "Standard Linear Regression Data Setup Functions Obtain Model Estimates Comparison Source", " Standard Linear Regression A standard regression model via maximum likelihood or least squares loss. Also examples for QR decomposition and normal equations. Can serve as an entry point for those starting out to the wider world of computational statistics as maximum likelihood is the fundamental approach used in most applied statistics, but which is also a key aspect of the Bayesian approach. Least squares loss is not confined to the standard lm setting, but is widely used in more predictive/‘algorithmic’ approaches e.g. in machine learning and elsewhere. Data Setup set.seed(123) # ensures replication # predictors and response N = 100 # sample size k = 2 # number of desired predictors X = matrix(rnorm(N * k), ncol = k) y = -.5 + .2*X[, 1] + .1*X[, 2] + rnorm(N, sd = .5) # increasing N will get estimated values closer to these dfXy = data.frame(X, y) Functions A maximum likelihood approach. lm_ML = function(par, X, y) { # par: parameters to be estimated # X: predictor matrix with intercept column # y: response # setup beta = par[-1] # coefficients sigma2 = par[1] # error variance sigma = sqrt(sigma2) N = nrow(X) # linear predictor LP = X %*% beta # linear predictor mu = LP # identity link in the glm sense # calculate likelihood L = dnorm(y, mean = mu, sd = sigma, log = TRUE) # log likelihood # L = -.5*N*log(sigma2) - .5*(1/sigma2)*crossprod(y-mu) # alternate log likelihood form -sum(L) # optim by default is minimization, and we want to maximize the likelihood # (see also fnscale in optim.control) } An approach via least squares loss function. lm_LS = function(par, X, y) { # arguments- # par: parameters to be estimated # X: predictor matrix with intercept column # y: response # setup beta = par # coefficients # linear predictor LP = X %*% beta # linear predictor mu = LP # identity link # calculate least squares loss function L = crossprod(y - mu) } Obtain Model Estimates Setup for use with optim. X = cbind(1, X) Initial values. Note we’d normally want to handle the sigma differently as it’s bounded by zero, but we’ll ignore for demonstration. Also sigma2 is not required for the LS approach as it is the objective function. init = c(1, rep(0, ncol(X))) names(init) = c(&#39;sigma2&#39;, &#39;intercept&#39;, &#39;b1&#39;, &#39;b2&#39;) optlmML = optim( par = init, fn = lm_ML, X = X, y = y, control = list(reltol = 1e-8) ) optlmLS = optim( par = init[-1], fn = lm_LS, X = X, y = y, control = list(reltol = 1e-8) ) pars_ML = optlmML$par pars_LS = c(sigma2 = optlmLS$value / (N - k - 1), optlmLS$par) # calculate sigma2 and add Comparison Compare to lm which uses QR decomposition. modlm = lm(y ~ ., dfXy) Example of QR. # QRX = qr(X) # Q = qr.Q(QRX) # R = qr.R(QRX) # Bhat = solve(R) %*% crossprod(Q, y) # alternate: qr.coef(QRX, y) round( rbind( pars_ML, pars_LS, modlm = c(summary(modlm)$sigma^2, coef(modlm))), digits = 3 ) sigma2 intercept b1 b2 pars_ML 0.219 -0.432 0.133 0.112 pars_LS 0.226 -0.432 0.133 0.112 modlm 0.226 -0.432 0.133 0.112 The slight difference in sigma is roughly maxlike dividing by N vs. N-k-1 in the traditional least squares approach; diminishes with increasing N as both tend toward whatever sd^2 you specify when creating the y response above. Compare to glm, which by default assumes gaussian family with identity link and uses lm.fit. modglm = glm(y ~ ., data = dfXy) summary(modglm) Call: glm(formula = y ~ ., data = dfXy) Deviance Residuals: Min 1Q Median 3Q Max -0.93651 -0.33037 -0.06222 0.31068 1.03991 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) -0.43247 0.04807 -8.997 1.97e-14 *** X1 0.13341 0.05243 2.544 0.0125 * X2 0.11191 0.04950 2.261 0.0260 * --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 (Dispersion parameter for gaussian family taken to be 0.2262419) Null deviance: 24.444 on 99 degrees of freedom Residual deviance: 21.945 on 97 degrees of freedom AIC: 140.13 Number of Fisher Scoring iterations: 2 Via normal equations. coefs = solve(t(X) %*% X) %*% t(X) %*% y # coefficients Compare. sqrt(crossprod(y - X %*% coefs) / (N - k - 1)) [,1] [1,] 0.4756489 summary(modlm)$sigma [1] 0.4756489 sqrt(modglm$deviance / modglm$df.residual) [1] 0.4756489 c(sqrt(pars_ML[1]), sqrt(pars_LS[1])) sigma2 sigma2 0.4684616 0.4756490 # rerun by adding 3-4 zeros to the N Source Base R source code found at https://github.com/m-clark/Miscellaneous-R-Code/blob/master/ModelFitting/standard_lm.R "],["logistic-regression.html", "Standard Logistic Data Setup Functions Obtain Model Estimates Comparison Source", " Standard Logistic A standard logistic regression model via maximum likelihood or exponential loss. Can serve as an entry point for those starting out to the wider world of computational statistics as maximum likelihood is the fundamental approach used in most applied statistics, but which is also a key aspect of the Bayesian approach. Exponential loss is not confined to the standard glm setting, but is widely used in more predictive/‘algorithmic’ approaches e.g. in machine learning and elsewhere. This follows the linear regression model approach. Data Setup Predictors and target. set.seed(1235) # ensures replication N = 10000 # sample size k = 2 # number of desired predictors X = matrix(rnorm(N * k), ncol = k) # the linear predictor lp = -.5 + .2 * X[, 1] + .1 * X[, 2] # increasing N will get estimated values closer to these y = rbinom(N, size = 1, prob = plogis(lp)) dfXy = data.frame(X, y) Functions A maximum likelihood approach. logreg_ML = function(par, X, y) { # arguments- # par: parameters to be estimated # X: predictor matrix with intercept column # y: response # setup beta = par # coefficients N = nrow(X) # linear predictor LP = X %*% beta # linear predictor mu = plogis(LP) # logit link # calculate likelihood L = dbinom(y, size = 1, prob = mu, log = TRUE) # log likelihood # L = y*log(mu) + (1 - y)*log(1-mu) # alternate log likelihood form -sum(L) # optim by default is minimization, and we want to maximize the likelihood # (see also fnscale in optim.control) } Another approach via exponential loss function. logreg_exp = function(par, X, y) { # arguments- # par: parameters to be estimated # X: predictor matrix with intercept column # y: response # setup beta = par # coefficients # linear predictor LP = X %*% beta # linear predictor # calculate exponential loss function (convert y to -1:1 from 0:1) L = sum(exp(-ifelse(y, 1, -1) * .5 * LP)) } Obtain Model Estimates Setup for use with optim. X = cbind(1, X) # initial values init = rep(0, ncol(X)) names(init) = c(&#39;intercept&#39;, &#39;b1&#39;, &#39;b2&#39;) optlmML = optim( par = init, fn = logreg_ML, X = X, y = y, control = list(reltol = 1e-8) ) optglmClass = optim( par = init, fn = logreg_exp, X = X, y = y, control = list(reltol = 1e-15) ) pars_ML = optlmML$par pars_exp = optglmClass$par Comparison Compare to glm. modglm = glm(y ~ ., dfXy, family = binomial) rbind( pars_ML, pars_exp, pars_GLM = coef(modglm) ) intercept b1 b2 pars_ML -0.5117658 0.2378927 0.08019841 pars_exp -0.5114284 0.2368478 0.07907056 pars_GLM -0.5117321 0.2378743 0.08032617 Source Base R source code found at https://github.com/m-clark/Miscellaneous-R-Code/blob/master/ModelFitting/standard_logistic.R "],["mixed-model-one-factor.html", "One-factor Mixed Model Main function Data set up Estimation Comparison Source", " One-factor Mixed Model An approach for one factor random effects model via maximum likelihood in R Matlab and Julia. It’s based on Statistical Modeling and Computation (2014) Chapter 10, example 10.10. Unfortunately I did this before knowing they had both matlab and R code on their website, though the R code here is a little cleaner and has comments. The data regards crop yield from 10 randomly selected locations and three collections at each. See one_factor_RE.m and one_factor_RE.jl for the related Matlab and Julia files, and the respective twofactorRE.* for the associated two factor random effects examples. Main function one_factor_re = function(mu, sigma2_mu, sigma2){ # Args - # mu: intercept # sigma2_mu: variance of intercept # sigma2: residual variance of y # I follow their notation for consistency d = nrow(y) ni = ncol(y) # covariance matrix of observations Sigmai = sigma2 * diag(ni) + sigma2_mu * matrix(1, ni, ni) # log likelihood l = rep(NA, 10) # iterate over the rows for(i in 1:d){ l[i] = .5 * t(y[i, ] - mu) %*% chol2inv(chol(Sigmai)) %*% (y[i, ] - mu) } ll = -(ni*d) / 2*log(2*pi) - d / 2*log(det(Sigmai)) - sum(l) return(-ll) } Data set up y = matrix(c(22.6,20.5,20.8, 22.6,21.2,20.5, 17.3,16.2,16.6, 21.4,23.7,23.2, 20.9,22.2,22.6, 14.5,10.5,12.3, 20.8,19.1,21.3, 17.4,18.6,18.6, 25.1,24.8,24.9, 14.9,16.3,16.6), 10, 3, byrow=T) Estimation Starting values starts = list( mu = mean(y), sigma2_mu = var(rowMeans(y)), sigma2 = mean(apply(y, 1, var)) ) Estimate one_factor_re(starts[[1]], starts[[2]], starts[[3]]) [1] 62.30661 Comparison Package bbmle has an mle2 function for maximum likelihood estimation based on underlying R functions like optim. LBFGS-B is used to place lower bounds on the variance estimates. library(bbmle) mlout = mle2( one_factor_re , start = starts, method = &#39;L-BFGS-B&#39;, lower = c( mu = -Inf, sigma2_mu = 0, sigma2 = 0 ), trace = T ) library(lme4) library(tidyverse) d = data.frame(y) %&gt;% pivot_longer(everything(), names_to = &#39;x&#39;, values_to = &#39;value&#39;) %&gt;% arrange(x) %&gt;% group_by(x) %&gt;% mutate(group = 1:n()) lme = lmer(value ~ 1 | group, data = d, REML = F) summary(mlout) Maximum likelihood estimation Call: mle2(minuslogl = one_factor_re, start = starts, method = &quot;L-BFGS-B&quot;, trace = T, lower = c(mu = -Inf, sigma2_mu = 0, sigma2 = 0)) Coefficients: Estimate Std. Error z value Pr(z) mu 19.60000 1.12173 17.4729 &lt; 2.2e-16 *** sigma2_mu 12.19400 5.62858 2.1664 0.030277 * sigma2 1.16667 0.36893 3.1623 0.001565 ** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 -2 log L: 124.5288 summary(lme) Linear mixed model fit by maximum likelihood [&#39;lmerMod&#39;] Formula: value ~ 1 | group Data: d AIC BIC logLik deviance df.resid 130.5 134.7 -62.3 124.5 27 Scaled residuals: Min 1Q Median 3Q Max -1.9950 -0.6555 0.1782 0.4870 1.7083 Random effects: Groups Name Variance Std.Dev. group (Intercept) 12.194 3.492 Residual 1.167 1.080 Number of obs: 30, groups: group, 10 Fixed effects: Estimate Std. Error t value (Intercept) 19.600 1.122 17.47 -2 * logLik(lme) &#39;log Lik.&#39; 124.5288 (df=3) Source Base R source code found at https://github.com/m-clark/Miscellaneous-R-Code/blob/master/ModelFitting/Mixed%20Models/one_factor_RE.R "],["mixed-model-two-factor.html", "Two-factor Mixed Model Main function Data set up Estimation Comparison Source", " Two-factor Mixed Model An approach for two factor random effects model via maximum likelihood in R Matlab and Julia. It’s based on Statistical Modeling and Computation (2014) Chapter 10, example 10.10. The data regards the breeding value of a set of five sires in raising pigs. Each sire is mated to a random group of dams, with the response being the average daily weight gain in pounds of two piglets in each litter. See one_factor_RE.R for a one factor model, and two_factor_RE.m and two_factor_RE.jl for the matlab and julia versions of this example. Note that the text has a typo on the sigma2 variance estimate (value should be .0023 not .023). Main function The function takes the log variances eta* as input to keep positive. two_factor_re = function(mu, eta_alpha, eta_gamma, eta) { # Args # mu: intercept # eta_alpha: random effect one # eta_gamma: random effect two # eta: residual variance of y sigma2_alpha = exp(eta_alpha) sigma2_gamma = exp(eta_gamma) sigma2 = exp(eta) n = length(y) # covariance matrix of observations Sigma = sigma2 * diag(n) + sigma2_alpha * tcrossprod(Xalpha) + sigma2_gamma * tcrossprod(Xgamma) # log likelihood ll = -n / 2 * log(2 * pi) - sum(log(diag(chol(Sigma)))) - .5 * t(y - mu) %*% chol2inv(chol(Sigma)) %*% (y - mu) return(-ll) } Data set up y = c(1.39,1.29,1.12,1.16,1.52,1.62,1.88,1.87,1.24,1.18, .95,.96,.82,.92,1.18,1.20,1.47,1.41,1.57,1.65) # for use in lme4, but also a more conceptual respresentation of the data d = expand.grid(sire = rep(1:5, 2), dam = 1:2) d = data.frame(d[order(d$sire), ], y) Estimation Starting values and test. starts = list( mu = mean(y), eta_alpha = var(tapply(y, d$sire, mean)), eta_gamma = var(y) / 3, eta = var(y) / 3 ) Xalpha = diag(5) %x% rep(1,4) Xgamma = diag(10) %x% rep(1,2) Estimation. two_factor_re(starts[[1]], starts[[2]], starts[[3]], starts[[4]]) [,1] [1,] 26.53887 Comparison library(bbmle) mlout = mle2(two_factor_re, start=starts, method=&#39;BFGS&#39;) ### lme4 comparison library(lme4) lme = lmer(y ~ (1 | sire) + (1 | dam:sire), d, REML = F) summary(mlout) Maximum likelihood estimation Call: mle2(minuslogl = two_factor_re, start = starts, method = &quot;BFGS&quot;) Coefficients: Estimate Std. Error z value Pr(z) mu 1.32000 0.11848 11.1410 &lt; 2.2e-16 *** eta_alpha -2.92393 0.84877 -3.4449 0.0005712 *** eta_gamma -3.44860 0.65543 -5.2616 1.428e-07 *** eta -6.07920 0.44721 -13.5935 &lt; 2.2e-16 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 -2 log L: -23.98631 exp(coef(mlout)[-1]) eta_alpha eta_gamma eta 0.05372198 0.03178996 0.00229000 summary(lme) Linear mixed model fit by maximum likelihood [&#39;lmerMod&#39;] Formula: y ~ (1 | sire) + (1 | dam:sire) Data: d AIC BIC logLik deviance df.resid -16 -12 12 -24 16 Scaled residuals: Min 1Q Median 3Q Max -1.21052 -0.59450 0.02314 0.61984 1.10386 Random effects: Groups Name Variance Std.Dev. dam:sire (Intercept) 0.03179 0.17830 sire (Intercept) 0.05372 0.23178 Residual 0.00229 0.04785 Number of obs: 20, groups: dam:sire, 10; sire, 5 Fixed effects: Estimate Std. Error t value (Intercept) 1.3200 0.1185 11.14 Source Base R source code found at https://github.com/m-clark/Miscellaneous-R-Code/blob/master/ModelFitting/Mixed%20Models/two_factor_RE.R "],["mixed-model-ML.html", "Mixed Model via ML Introduction Maximum Likelihood Estimation Source", " Mixed Model via ML Introduction The following is based on the @Wood text on additive models, chapter 6 in particular. It assumes familiarity with standard regression from a matrix perspective and at least passing familiarity with mixed models. The full document this chapter is based on can be found here, and contains more detail and exposition. Maximum Likelihood Estimation For this we’ll use the sleepstudy data from the lme4 package. The data has reaction times for 18 individuals over 10 days each (see the help file for the sleepstudy object for more details). Data data(sleepstudy, package = &#39;lme4&#39;) X = model.matrix( ~ Days, sleepstudy) Z = model.matrix( ~ factor(sleepstudy$Subject) - 1) colnames(Z) = paste0(&#39;Subject_&#39;, unique(sleepstudy$Subject)) # for cleaner presentation later rownames(Z) = paste0(&#39;Subject_&#39;, sleepstudy$Subject) y = sleepstudy$Reaction ML function The following is based on the code in Wood (6.2.2), with a couple modifications for consistent nomenclature and presentation. We use optim and a minimizing function, in this case the negative log likelihood, to estimate the parameters of interest, collectively \\(\\theta\\), in the code below. The (square root of the) variances will be estimated on the log scale. In Wood, he simply extracts the ‘fixed effects’ for the intercept and days effects using lm (6.2.3), and we’ll do the same. llMixed = function(y, X, Z, theta){ tau = exp(theta[1]) sigma = exp(theta[2]) n = length(y) # evaluate covariance matrix for y e = tcrossprod(Z)*tau^2 + diag(n)*sigma^2 L = chol(e) # L&#39;L = e # transform dependent linear model to independent y = backsolve(L, y, transpose=TRUE) X = backsolve(L, X, transpose=TRUE) b = coef(lm(y~X-1)) LP = X %*% b ll = -n/2*log(2*pi) -sum(log(diag(L))) - crossprod(y-LP)/2 -ll } Here is an alternative function using a multivariate normal approach that doesn’t use the transformation to independent, and might provide additional perspective. llMixedMV = function(y, X, Z, theta){ tau = exp(theta[1]) sigma = exp(theta[2]) n = length(y) # evaluate covariance matrix for y e = tcrossprod(Z)*tau^2 + diag(n)*sigma^2 b = coef(lm.fit(X, y)) mu = X %*% b ll = mvtnorm::dmvnorm(y, mu, e, log=T) -ll } Results Now we’re ready to use the optim function for estimation. A slight change to tolerance is included to get closer estimates to lme4, which we will compare shortly. paramInit = c(0, 0) names(paramInit) = c(&#39;tau&#39;, &#39;sigma&#39;) modelResults = optim( fn = llMixed, X = X, y = y, Z = Z, par = paramInit, control = list(reltol = 1e-10) ) modelResultsMV = optim( fn = llMixedMV, X = X, y = y, Z = Z, par = paramInit, control = list(reltol = 1e-10) ) rbind( c(exp(modelResults$par), negLogLik = modelResults$value, coef(lm(y ~ X - 1))), c(exp(modelResultsMV$par), negLogLik = modelResultsMV$value, coef(lm(y ~ X - 1))) ) %&gt;% round(2) tau sigma negLogLik X(Intercept) XDays [1,] 36.02 30.9 897.04 251.41 10.47 [2,] 36.02 30.9 897.04 251.41 10.47 As we can see, both formulations produce identical results. We can now compare those results to the lme4 output for the same model, and see that we’re getting what we should. library(lme4) lmeMod = lmer(Reaction ~ Days + (1|Subject), sleepstudy, REML = FALSE) lmeMod Linear mixed model fit by maximum likelihood [&#39;lmerMod&#39;] Formula: Reaction ~ Days + (1 | Subject) Data: sleepstudy AIC BIC logLik deviance df.resid 1802.0786 1814.8505 -897.0393 1794.0786 176 Random effects: Groups Name Std.Dev. Subject (Intercept) 36.01 Residual 30.90 Number of obs: 180, groups: Subject, 18 Fixed Effects: (Intercept) Days 251.41 10.47 We can also predict the random effects (Wood, 6.2.4), and after doing so again compare the results to the lme4 estimates. tau = exp(modelResults$par)[1] tausq = tau^2 sigma = exp(modelResults$par)[2] sigmasq = sigma^2 Sigma = tcrossprod(Z)*tausq/sigmasq + diag(length(y)) ranefEstimated = tausq*t(Z)%*%solve(Sigma) %*% resid(lm(y~X-1))/sigmasq data.frame( ranefEstimated, lme4 = ranef(lmeMod)$Subject[[1]] ) %&gt;% round(2) ranefEstimated lme4 Subject_308 40.64 40.64 Subject_309 -77.57 -77.57 Subject_310 -62.88 -62.88 Subject_330 4.39 4.39 Subject_331 10.18 10.18 Subject_332 8.19 8.19 Subject_333 16.44 16.44 Subject_334 -2.99 -2.99 Subject_335 -45.12 -45.12 Subject_337 71.92 71.92 Subject_349 -21.12 -21.12 Subject_350 14.06 14.06 Subject_351 -7.83 -7.83 Subject_352 36.25 36.25 Subject_369 7.01 7.01 Subject_370 -6.34 -6.34 Subject_371 -3.28 -3.28 Subject_372 18.05 18.05 Issues with ML estimation Situations arise in which using maximum likelihood for mixed models would result in notably biased estimates (e.g. small N, lots of fixed effects), and so it is typically not used. Standard software usually defaults to restricted maximum likelihood. However, our purpose here has been served, so we will not dwell further on mixed model estimation. Link with penalized regression A link exists between mixed models and a penalized likelihood approach. For a penalized approach with the SLiM, the objective function we want to minimize can be expressed as follows: \\[ \\lVert y- X\\beta \\rVert^2 + \\beta^\\intercal\\beta \\] The added component to the sum of the squared residuals is the penalty. By adding the sum of the squared coefficients, we end up keeping them from getting too big, and this helps to avoid overfitting. Another interesting aspect of this approach is that it is comparable to using a specific prior on the coefficients in a Bayesian framework. We can now see mixed models as a penalized technique. If we knew \\(\\sigma\\) and \\(\\psi_\\theta\\), then the predicted random effects \\(g\\) and estimates for the fixed effects \\(\\beta\\) are those that minimize the following objective function: \\[ \\frac{1}{\\sigma^2}\\lVert y - X\\beta - Zg \\rVert^2 + g^\\intercal\\psi_\\theta^{-1}g \\] Source Main doc found at https://m-clark.github.io/docs/mixedModels/mixedModelML.html "],["probit.html", "Probit &amp; Bivariate Probit standard probit Bivariate probit", " Probit &amp; Bivariate Probit Stata’s seems to be the primary audience concerned with these models, but I thought I’d play around with one here (I’ve never had reason to use a probit model in practice). Stata examples come from the UCLA ATS website and the Stata manual so one can investigate the Stata result for comparison. standard probit probitLL = function(beta, X, y){ mu = X %*% beta # these produce identical results, but the second is the typical depiction ll = sum(dbinom( y, size = 1, prob = pnorm(mu), log = T )) # ll = sum(y * pnorm(mu, log = T) + (1 - y) * log(1 - pnorm(mu))) -ll } Examples Example 1 available at https://stats.idre.ucla.edu/stata/dae/probit-regression/ admit = haven::read_dta(&#39;https://stats.idre.ucla.edu/stat/stata/dae/binary.dta&#39;) head(admit) # A tibble: 6 x 4 admit gre gpa rank &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 0 380 3.61 3 2 1 660 3.67 3 3 1 800 4 1 4 1 640 3.19 4 5 0 520 2.93 4 6 1 760 3 2 X = model.matrix(admit~ gre + gpa + factor(rank), admit) y = admit$admit init = rep(0, ncol(X)) optimResult = optim( fn = probitLL, par = init, X = X, y = y, method = &#39;BFGS&#39; ) optimResult $par [1] -2.2518260555 0.0007915268 0.5453357468 -0.4211611887 -0.8285356685 -0.9457812896 $value [1] 229.6227 $counts function gradient 140 11 $convergence [1] 0 $message NULL Example 2 from Stata manual on standard probit We have data on the make, weight, and mileage rating of 22 foreign and 52 domestic automobiles. We wish to fit a probit model explaining whether a car is foreign based on its weight and mileage.\" auto = haven::read_dta(&#39;http://www.stata-press.com/data/r13/auto.dta&#39;) head(auto) # A tibble: 6 x 12 make price mpg rep78 headroom trunk weight length turn displacement gear_ratio foreign &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl+lbl&gt; 1 AMC Concord 4099 22 3 2.5 11 2930 186 40 121 3.58 0 [Domestic] 2 AMC Pacer 4749 17 3 3 11 3350 173 40 258 2.53 0 [Domestic] 3 AMC Spirit 3799 22 NA 3 12 2640 168 35 121 3.08 0 [Domestic] 4 Buick Century 4816 20 3 4.5 16 3250 196 40 196 2.93 0 [Domestic] 5 Buick Electra 7827 15 4 4 20 4080 222 43 350 2.41 0 [Domestic] 6 Buick LeSabre 5788 18 3 4 21 3670 218 43 231 2.73 0 [Domestic] X = model.matrix(foreign~ weight + mpg, auto) y = auto$foreign init = rep(0, ncol(X)) optimResult = optim( fn = probitLL, par = init, X = X, y = y ) optimResult $par [1] 8.277015097 -0.002335939 -0.103973147 $value [1] 26.84419 $counts function gradient 380 NA $convergence [1] 0 $message NULL Bivariate probit Main function. bivariateProbitLL = function(pars, X, y1, y2) { rho = pars[1] mu1 = X %*% pars[2:(ncol(X) + 1)] mu2 = X %*% pars[(ncol(X) + 2):length(pars)] q1 = ifelse(y1 == 1, 1,-1) q2 = ifelse(y2 == 1, 1,-1) require(mnormt) eta1 = q1 * mu1 eta2 = q2 * mu2 ll = matrix(NA, nrow = nrow(X)) for (i in 1:length(ll)) { corr = q1[i] * q2[i] * rho corr = matrix(c(1, corr, corr, 1), 2) ll[i] = log( pmnorm( x = c(eta1[i], eta2[i]), mean = c(0, 0), varcov = corr ) ) } # the loop is probably clearer, and there is no difference in time, but here&#39;s # a oneliner ll = mapply(function(e1, e2, q1, q2) log(pmnorm(x = c(e1, e2), # varcov = matrix(c(1,q1*q2*rho,q1*q2*rho,1),2))), eta1, eta2, q1, q2) -sum(ll) } Example From stata manual on bivariate probit: We wish to model the bivariate outcomes of whether children attend private school and whether the head of the household voted for an increase in property tax based on the other covariates. school = haven::read_dta(&#39;http://www.stata-press.com/data/r13/school.dta&#39;) head(school) # A tibble: 6 x 11 obs pub12 pub34 pub5 private years school loginc logptax vote logeduc &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 1 0 1 0 0 10 1 9.77 7.05 1 7.21 2 2 0 1 0 0 8 0 10.0 7.05 0 7.61 3 3 1 0 0 0 4 0 10.0 7.05 0 8.28 4 4 0 1 0 0 13 0 9.43 6.40 0 6.82 5 5 0 1 0 0 3 1 10.0 7.28 1 7.69 6 6 1 0 0 0 5 0 10.5 7.05 0 6.91 X = model.matrix(private ~ years + logptax + loginc, school) y1 = school$private y2 = school$vote init = c(0, rep(0, ncol(X)*2)) # you&#39;ll probably get a warning or two, ignore; takes a couple seconds optimResult = optim( fn = bivariateProbitLL, par = init, X = X, y1 = y1, y2 = y2, method = &#39;BFGS&#39; ) loglik = optimResult$value rho = optimResult$par[1] coefsPrivate = optimResult$par[2:(ncol(X) + 1)] coefsVote = optimResult$par[(ncol(X) + 2):length(init)] names(coefsPrivate) = names(coefsVote) = c(&#39;Int&#39;, &#39;years&#39;, &#39;logptax&#39;, &#39;loginc&#39;) list( loglik = loglik, rho = rho, Private = coefsPrivate, Vote = coefsVote ) $loglik [1] 89.25407 $rho [1] -0.2695802 $Private Int years logptax loginc -4.14327955 -0.01193699 -0.11030513 0.37459892 $Vote Int years logptax loginc -0.52933721 -0.01686685 -1.28983223 0.99840976 "],["heckman-selection.html", "Heckman Selection Data Setup 2 step approach Maximum Likelihood", " Heckman Selection Based on bleven’s example here: https://www3.nd.edu/~wevans1/ecoe60303/sample_selection_example.ppt, which is more or less the ‘classic’ example, variations of which you’ll find all over regarding women’s wages. Description of the data: Draw 10,000 obs at random educ uniform over [0,16] age uniform over [18,64] wearnl = 4.49 + 0.08 * educ + 0.012 * age + ε Generate missing data for wearnl drawn z from standard normal [0,1]. z is actually never explained in the slides, I think it’s left out on slide 3 and just represents an additional covariate. d*=-1.5+0.15*educ+0.01*age+0.15*z+v wearnl missing if d*≤0 wearn reported if d*&gt;0 wearnl_all = wearnl with non-missing obs Data Setup set.seed(123456) N = 10000 educ = sample(1:16, N, replace = T) age = sample(18:64, N, replace = T) covmat = matrix(c(.46^2, .25*.46, .25*.46, 1), ncol = 2) errors = mvtnorm::rmvnorm(N, sigma = covmat) z = rnorm(N) e = errors[, 1] v = errors[, 2] wearnl = 4.49 + .08 * educ + .012 * age + e d_star = -1.5 + 0.15 * educ + 0.01 * age + 0.15 * z + v observed_index = d_star &gt; 0 d = data.frame(wearnl, educ, age, z, observed_index) Examine linear regression approaches if desired. # lm based on full data lm_all = lm(wearnl ~ educ + age, data=d) # lm based on observed data lm_obs = lm(wearnl ~ educ + age, data=d[observed_index,]) summary(lm_all) Call: lm(formula = wearnl ~ educ + age, data = d) Residuals: Min 1Q Median 3Q Max -2.03286 -0.31240 0.00248 0.31578 1.50828 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 4.4691266 0.0171413 260.72 &lt;2e-16 *** educ 0.0798814 0.0010005 79.84 &lt;2e-16 *** age 0.0124381 0.0003398 36.60 &lt;2e-16 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 0.4611 on 9997 degrees of freedom Multiple R-squared: 0.4331, Adjusted R-squared: 0.433 F-statistic: 3820 on 2 and 9997 DF, p-value: &lt; 2.2e-16 summary(lm_obs) # smaller coefs, resid standard error Call: lm(formula = wearnl ~ educ + age, data = d[observed_index, ]) Residuals: Min 1Q Median 3Q Max -1.75741 -0.30289 -0.00133 0.30918 1.50032 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 4.6713823 0.0258865 180.46 &lt;2e-16 *** educ 0.0705849 0.0014760 47.82 &lt;2e-16 *** age 0.0114758 0.0004496 25.52 &lt;2e-16 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 0.4507 on 5517 degrees of freedom Multiple R-squared: 0.3374, Adjusted R-squared: 0.3372 F-statistic: 1405 on 2 and 5517 DF, p-value: &lt; 2.2e-16 2 step approach The two-step approach first conducts a probit model regarding whether the individual is observed or not, in order to calculate the inverse mills ratio, or ‘nonselection hazard’. The second step is a standard lm. Step 1: Probit Model probit = glm(observed_index ~ educ + age + z, data = d, family = binomial(link = &#39;probit&#39;)) summary(probit) Call: glm(formula = observed_index ~ educ + age + z, family = binomial(link = &quot;probit&quot;), data = d) Deviance Residuals: Min 1Q Median 3Q Max -2.4674 -0.9062 0.4628 0.8800 2.2674 Coefficients: Estimate Std. Error z value Pr(&gt;|z|) (Intercept) -1.519248 0.052819 -28.763 &lt;2e-16 *** educ 0.150027 0.003220 46.588 &lt;2e-16 *** age 0.010072 0.001015 9.926 &lt;2e-16 *** z 0.159292 0.013889 11.469 &lt;2e-16 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 (Dispersion parameter for binomial family taken to be 1) Null deviance: 13755 on 9999 degrees of freedom Residual deviance: 11119 on 9996 degrees of freedom AIC: 11127 Number of Fisher Scoring iterations: 4 # http://www.stata.com/support/faqs/statistics/inverse-mills-ratio/ probit_lp = predict(probit) mills0 = dnorm(probit_lp)/pnorm(probit_lp) summary(mills0) Min. 1st Qu. Median Mean 3rd Qu. Max. 0.07588 0.38632 0.70027 0.75664 1.09246 1.96602 # identical formulation # probit_lp = -predict(probit) # imr = dnorm(probit_lp)/(1-pnorm(probit_lp)) imr = mills0[observed_index] summary(imr) Min. 1st Qu. Median Mean 3rd Qu. Max. 0.07588 0.28739 0.48466 0.57015 0.77617 1.87858 Take a look at the distribution. ggplot2::qplot(imr, geom = &#39;histogram&#39;) Step 2: Estimate via linear regression Standard regression model using the inverse mills ratio as covariate lm_select = lm(wearnl ~ educ + age + imr, data = d[observed_index, ]) summary(lm_select) Call: lm(formula = wearnl ~ educ + age + imr, data = d[observed_index, ]) Residuals: Min 1Q Median 3Q Max -1.75994 -0.30293 -0.00186 0.31049 1.48179 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 4.5159161 0.1063144 42.477 &lt;2e-16 *** educ 0.0782580 0.0052989 14.769 &lt;2e-16 *** age 0.0119700 0.0005564 21.513 &lt;2e-16 *** imr 0.0955209 0.0633557 1.508 0.132 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 0.4506 on 5516 degrees of freedom Multiple R-squared: 0.3377, Adjusted R-squared: 0.3373 F-statistic: 937.4 on 3 and 5516 DF, p-value: &lt; 2.2e-16 Compare to sampleSelection package. library(sampleSelection) selection_2step = selection(observed_index ~ educ + age + z, wearnl ~ educ + age, method = &#39;2step&#39;) summary(selection_2step) -------------------------------------------- Tobit 2 model (sample selection model) 2-step Heckman / heckit estimation 10000 observations (4480 censored and 5520 observed) 10 free parameters (df = 9991) Probit selection equation: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) -1.519248 0.052725 -28.815 &lt;2e-16 *** educ 0.150027 0.003221 46.577 &lt;2e-16 *** age 0.010072 0.001014 9.934 &lt;2e-16 *** z 0.159292 0.013937 11.430 &lt;2e-16 *** Outcome equation: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 4.5159161 0.1066914 42.33 &lt;2e-16 *** educ 0.0782580 0.0053181 14.71 &lt;2e-16 *** age 0.0119700 0.0005592 21.41 &lt;2e-16 *** Multiple R-Squared:0.3377, Adjusted R-Squared:0.3373 Error terms: Estimate Std. Error t value Pr(&gt;|t|) invMillsRatio 0.09552 0.06354 1.503 0.133 sigma 0.45550 NA NA NA rho 0.20970 NA NA NA -------------------------------------------- coef(lm_select)[&#39;imr&#39;] / summary(lm_select)$sigma # slightly off imr 0.2119813 coef(lm_select)[&#39;imr&#39;] / summary(selection_2step)$estimate[&#39;sigma&#39;, &#39;Estimate&#39;] imr 0.2097041 Maximum Likelihood The following likelihood function takes arguments as follows: - par: the regression coefficients pertaining to the two models, the residual se - sigma and rho for the correlation estimate - X: observed data model matrix for the linear regression model - Z: complete data model matrix for the probit model - y: the target variable - observed_index: an index denoting whether y is observed select_ll &lt;- function(par, X, Z, y, observed_index) { gamma = par[1:4] lp_probit = Z %*% gamma beta = par[5:7] lp_lm = X %*% beta sigma = par[8] rho = par[9] ll = sum(log(1-pnorm(lp_probit[!observed_index]))) + - log(sigma) + sum(dnorm(y, mean = lp_lm, sd = sigma, log = TRUE)) + sum( pnorm((lp_probit[observed_index] + rho/sigma * (y-lp_lm)) / sqrt(1-rho^2), log.p = TRUE) ) - ll } X = model.matrix(lm_select) Z = model.matrix(probit) # initial values init = c(coef(probit), coef(lm_select)[-4], 1, 0) Estimate via optim. Without bounds for sigma and rho you’ll get warnings, but does fine anyway heckman_ml_unbounded = optim( init, select_ll, X = X[, -4], Z = Z, y = wearnl[observed_index], observed_index = observed_index, method = &#39;BFGS&#39;, control = list(maxit = 1000, reltol = 1e-15), hessian = T ) heckman_ml_bounded = optim( init, select_ll, X = X[, -4], Z = Z, y = wearnl[observed_index], observed_index = observed_index, method = &#39;L-BFGS&#39;, lower = c(rep(-Inf, 7), 1e-10,-1), upper = c(rep(Inf, 8), 1), control = list(maxit = 1000, factr = 1e-15), hessian = T ) Comparison Comparison model. selection_ml = selection(observed_index ~ educ + age + z, wearnl ~ educ + age, method = &#39;ml&#39;) # summary(selection_ml) library(tidyverse) # compare coefficients tibble( model = rep(c(&#39;probit&#39;, &#39;lm&#39;, &#39;both&#39;), c(4, 4, 1)), par = names(coef(selection_ml)), sampselpack_ml = coef(selection_ml), unbounded_ml = heckman_ml_unbounded$par, bounded_ml = heckman_ml_bounded$par, explicit_twostep = c( coef(probit), coef(lm_select)[1:3], summary(lm_select)$sigma, coef(lm_select)[&#39;imr&#39;] / summary(lm_select)$sigma ), sampselpack_2step = coef(selection_2step)[-8] ) %&gt;% mutate_if(is.numeric, round, digits = 3) # A tibble: 9 x 7 model par sampselpack_ml unbounded_ml bounded_ml explicit_twostep sampselpack_2step &lt;chr&gt; &lt;chr&gt; &lt;cf.slctn&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 probit (Intercept) -1.520 -1.52 -1.52 -1.52 -1.52 2 probit educ 0.150 0.15 0.15 0.15 0.15 3 probit age 0.010 0.01 0.01 0.01 0.01 4 probit z 0.157 0.158 0.158 0.159 0.159 5 lm (Intercept) 4.478 4.48 4.48 4.52 4.52 6 lm educ 0.080 0.08 0.08 0.078 0.078 7 lm age 0.012 0.012 0.012 0.012 0.012 8 lm sigma 0.458 0.458 0.458 0.451 0.456 9 both rho 0.259 0.257 0.255 0.212 0.21 # compare standard errors tibble( model = rep(c(&#39;probit&#39;, &#39;lm&#39;, &#39;both&#39;), c(4, 4, 1)), par = names(coef(selection_ml)), sampselpack_ml = sqrt(diag(solve( -selection_ml$hessian ))), unbounded_ml = sqrt(diag(solve( heckman_ml_unbounded$hessian ))), bounded_ml = sqrt(diag(solve( heckman_ml_bounded$hessian ))), explicit_twostep = c( summary(probit)$coefficients[, 2], summary(lm_select)$coefficients[-4, 2], NA, NA ), sampselpack_2step = summary(selection_2step)$estimate[-8, 2] ) %&gt;% mutate(across(where(is.numeric), round, digits = 3)) # A tibble: 9 x 7 model par sampselpack_ml unbounded_ml bounded_ml explicit_twostep sampselpack_2step &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 probit (Intercept) 0.053 0.053 0.053 0.053 0.053 2 probit educ 0.003 0.003 0.003 0.003 0.003 3 probit age 0.001 0.001 0.001 0.001 0.001 4 probit z 0.014 0.014 0.014 0.014 0.014 5 lm (Intercept) 0.09 0.09 0.09 0.106 0.107 6 lm educ 0.004 0.004 0.005 0.005 0.005 7 lm age 0.001 0.001 0.001 0.001 0.001 8 lm sigma 0.008 0.008 0.008 NA NA 9 both rho 0.112 0.112 0.112 NA NA "],["tobit.html", "Tobit Demonstrate Censoring with an Upper Limit Demonstrate censoring with a Lower Limit", " Tobit A simple demonstration of tobit regression via maximum likelihood. The issue is one where data is censored such that while we observe the value, it is not the true value, which would extend beyond the range of the observed data. This is very commonly seen in cases where the dependent variable has been given some arbitrary cutoff at the lower or upper end of the range, often resulting in floor or ceiling effects respectively. The conceptual idea is that we are interested in modeling the underlying latent variable that would not have suchrestriction if it was actually observed. tobit &lt;- function(par, X, y, ul = -Inf, ll = Inf) { # this function only takes a lower OR upper limit # parameters sigma = exp(par[length(par)]) beta = par[-length(par)] # create indicator depending on chosen limit if (!is.infinite(ll)) { limit = ll indicator = y &gt; ll } else { limit = ul indicator = y &lt; ul } # linear predictor lp = X %*% beta # log likelihood ll = sum(indicator * log((1/sigma)*dnorm((y-lp)/sigma)) ) + sum((1-indicator) * log(pnorm((lp-limit)/sigma, lower=is.infinite(ll)))) -ll } Demonstrate Censoring with an Upper Limit Data setup Data regards academic aptitude (GRE scores) with will be modeled using reading and math test scores, as well as the type of program the student is enrolled in (academic, general, or vocational). See this for an applied example and more detail- https://stats.idre.ucla.edu/r/dae/tobit-models/ library(tidyverse) acad_apt = read_csv(&quot;https://stats.idre.ucla.edu/stat/data/tobit.csv&quot;) %&gt;% mutate(prog = factor(prog, labels = c(&#39;acad&#39;, &#39;general&#39;, &#39;vocational&#39;))) Setup data and initial values. initmod = lm(apt ~ read + math + prog, data = acad_apt) X = model.matrix(initmod) init = c(coef(initmod), log_sigma = log(summary(initmod)$sigma)) Estimate via optim. res = optim( par = init, tobit, y = acad_apt$apt, X = X, ul = 800, method = &#39;BFGS&#39;, control = list(maxit = 2000, reltol = 1e-15) ) # this would be more akin to the default Stata default approach # optim( # par = init, # tobit, # y = acad_apt$apt, # X = X, # ul = 800, # control = list(maxit = 16000, reltol = 1e-15) # ) Comparison Compare to AER package tobit function. library(survival) aer_mod = AER::tobit( apt ~ read + math + prog, data = acad_apt, left = -Inf, right = 800 ) rbind( tobit = c( res$par[1:5], sigma = exp(res$par[6]), logLike = -res$value ), AER = c(coef(aer_mod), aer_mod$scale, logLik(aer_mod)) ) %&gt;% round(3) (Intercept) read math proggeneral progvocational sigma.log_sigma logLike tobit 209.566 2.698 5.914 -12.716 -46.143 65.677 -1041.063 AER 209.566 2.698 5.914 -12.715 -46.144 65.677 -1041.063 AER is actually just using survreg from the survival package. Survival models are usually for modeling time to some event, e.g. death in medical studies, and the censoring comes from the fact that the observed event does not occur for some people. Like our tobit function, an indicator is needed to denote who is or isn’t censored. In survival models, the indicator is for the event itself, and means they are NOT censored. So we’ll reverse the indicator used in the tobit function for survreg. surv_mod = survreg(Surv(apt, apt &lt; 800, type = &#39;right&#39;) ~ read + math + prog, data = acad_apt, dist = &#39;gaussian&#39;) Compare all results. rbind( tobit = c( res$par[1:5], sigma = exp(res$par[6]), logLike = -res$value ), AER = c(coef(aer_mod), aer_mod$scale, logLik(aer_mod)), survival = c(coef(surv_mod), surv_mod$scale, logLik(surv_mod)) ) %&gt;% round(3) (Intercept) read math proggeneral progvocational sigma.log_sigma logLike tobit 209.566 2.698 5.914 -12.716 -46.143 65.677 -1041.063 AER 209.566 2.698 5.914 -12.715 -46.144 65.677 -1041.063 survival 209.566 2.698 5.914 -12.715 -46.144 65.677 -1041.063 Demonstrate censoring with a Lower Limit Create a censored data situation for the low end. The scale itself would be censored for anyone scoring a 200, but that basically doesn’t happen. In this data, 15 are less than a score of 500, so we’ll do that. acad_apt = acad_apt %&gt;% mutate(apt2 = apt, apt2 = if_else(apt2 &lt; 500, 500, apt2)) Estimate and use AER for comparison. res = optim( par = init, tobit, y = acad_apt$apt2, X = X, ll = 400, method = &#39;BFGS&#39;, control = list(maxit = 2000, reltol = 1e-15) ) aer_mod = AER::tobit(apt2 ~ read + math + prog, data = acad_apt, left = 400) Comparison rbind( tobit = c( res$par[1:5], sigma = exp(res$par[6]), logLike = -res$value ), AER = c(coef(aer_mod), aer_mod$scale, logLik(aer_mod)) ) %&gt;% round(3) (Intercept) read math proggeneral progvocational sigma.log_sigma logLike tobit 270.408 2.328 5.086 -11.331 -38.606 57.024 -1092.483 AER 270.409 2.328 5.085 -11.331 -38.606 57.024 -1092.483 "],["naive-bayes.html", "Naive Bayes Initialization Comparison", " Naive Bayes Initialization Demo for binary data. First we generate some data. We gave several binary covariates and a binary target variable y. set.seed(123) x = matrix(sample(0:1, 50, replace = TRUE), ncol = 5) xf = data.frame(lapply(data.frame(x), factor)) y = sample(0:1, 10, prob = c(.25, .75), replace = TRUE) Comparison We can use e1071 for comparison. library(e1071) m = naiveBayes(xf, y) m Naive Bayes Classifier for Discrete Predictors Call: naiveBayes.default(x = xf, y = y) A-priori probabilities: y 0 1 0.3 0.7 Conditional probabilities: X1 y 0 1 0 0.6666667 0.3333333 1 0.5714286 0.4285714 X2 y 0 1 0 0.6666667 0.3333333 1 0.4285714 0.5714286 X3 y 0 1 0 0.6666667 0.3333333 1 0.5714286 0.4285714 X4 y 0 1 0 1.0000000 0.0000000 1 0.4285714 0.5714286 X5 y 0 1 0 0.3333333 0.6666667 1 0.8571429 0.1428571 Using base R for our model, we can easily obtain the ‘predictions’… lapply(xf, function(var) t(prop.table(table(&#39; &#39; = var, y), margin = 2))) $X1 y 0 1 0 0.6666667 0.3333333 1 0.5714286 0.4285714 $X2 y 0 1 0 0.6666667 0.3333333 1 0.4285714 0.5714286 $X3 y 0 1 0 0.6666667 0.3333333 1 0.5714286 0.4285714 $X4 y 0 1 0 1.0000000 0.0000000 1 0.4285714 0.5714286 $X5 y 0 1 0 0.3333333 0.6666667 1 0.8571429 0.1428571 "],["penalized-maximum-likelihood.html", "Penalized Maximum Likelihood Data Setup Functions Obtain Model Estimates Comparison Source", " Penalized Maximum Likelihood A standard regression model via penalized likelihood. See the standard_lm.R code for comparison. Here the penalty is specified (via lambda argument) but one would typically estimate via cross-validation or some other fashion. Two penalties are possible with the function. One using the (squared) L2 norm (aka ridge regression, tikhonov regularization), another using the L1 norm (aka lasso) which has the possibility of penalizing coefficients to zero, and thus can serve as a model selection procedure. I have a more technical appraoch to the lasso in the lasso.R file. Note that both L2 and L1 approaches can be seen as maximum a posteriori (MAP) estimates for a Bayesian regression with a specific prior on the coefficients. The L2 approach is akin to a normal prior with zero mean, while L1 is akin to a zero mean Laplace prior. See the Bayesian scripts for ways to implement. Data Setup set.seed(123) # ensures replication # predictors and response N = 100 # sample size k = 2 # number of desired predictors X = matrix(rnorm(N * k), ncol = k) y = -.5 + .2*X[, 1] + .1*X[, 2] + rnorm(N, sd = .5) # increasing N will get estimated values closer to these dfXy = data.frame(X,y) Functions A maximum likelihood approach. penalized_ML = function(par, X, y, lambda = .1, type = &#39;L2&#39;) { # arguments- # par: parameters to be estimated # X: predictor matrix with intercept column # y: response # lambda: penalty coefficient # type: penalty approach # setup beta = par[-1] # coefficients sigma2 = par[1] # error variance sigma = sqrt(sigma2) N = nrow(X) # linear predictor LP = X %*% beta # linear predictor mu = LP # identity link in the glm sense # calculate likelihood L = dnorm(y, mean = mu, sd = sigma, log = T) # log likelihood PL = switch(type, &#39;L2&#39; = -sum(L) + lambda * crossprod(beta[-1]), # the intercept is not penalized &#39;L1&#39; = -sum(L) + lambda * sum(abs(beta[-1])) ) } glmnet style approach that will put the lambda coefficient on equivalent scale. Uses a different objective function. Note that glmnet is actually elasticnet and mixes both L1 and L2 penalties. penalized_ML2 = function(par, X, y, lambda = .1, type = &#39;L2&#39;) { # arguments- # par: parameters to be estimated # X: predictor matrix with intercept column # y: response # lambda: penalty coefficient # type: penalty approach # setup beta = par # coefficients N = nrow(X) # linear predictor LP = X %*% beta # linear predictor mu = LP # identity link in the glm sense obj = switch(type, &#39;L2&#39; = .5*crossprod(y - X %*% beta)/N + lambda * crossprod(beta[-1]), &#39;L1&#39; = .5*crossprod(y - X %*% beta)/N + lambda * sum(abs(beta[-1])) ) } Obtain Model Estimates Setup the model matrix for use with optim. X = cbind(1, X) Initial values. note we’d normally want to handle the sigma differently as it’s bounded by zero, but we’ll ignore for demonstration. Also sigma2 is not required for the LS approach. init = c(1, rep(0, ncol(X))) names(init) = c(&#39;sigma2&#39;, &#39;intercept&#39;, &#39;b1&#39;, &#39;b2&#39;) optlmpenalized_MLL2 = optim( par = init, fn = penalized_ML, X = X, y = y, lambda = 1, control = list(reltol = 1e-12) ) optlmpenalized_MLL1 = optim( par = init, fn = penalized_ML, X = X, y = y, lambda = 1, type = &#39;L1&#39;, control = list(reltol = 1e-12) ) parspenalized_MLL2 = optlmpenalized_MLL2$par parspenalized_MLL1 = optlmpenalized_MLL1$par Comparison Compare to lm. modlm = lm(y ~ ., dfXy) round( rbind( parspenalized_MLL2, parspenalized_MLL1, modlm = c(summary(modlm)$sigma ^ 2, coef(modlm)) ), digits = 4 ) sigma2 intercept b1 b2 parspenalized_MLL2 0.2195 -0.4325 0.1327 0.1113 parspenalized_MLL1 0.2195 -0.4325 0.1306 0.1094 modlm 0.2262 -0.4325 0.1334 0.1119 Compare to glmnet. Setting alpha to 0 and 1 is equivalent to L2 and L1 penalties respectively. You also wouldn’t want to specify lambda normally, and rather let it come about as part of the estimation procedure. We do so here just for demonstration. library(glmnet) glmnetL2 = glmnet( X[, -1], y, alpha = 0, lambda = .01, standardize = FALSE ) glmnetL1 = glmnet( X[, -1], y, alpha = 1, lambda = .01, standardize = FALSE ) pars_L2 = optim( par = init[-1], fn = penalized_ML2, X = X, y = y, lambda = .01, control = list(reltol = 1e-12) )$par pars_L1 = optim( par = init[-1], fn = penalized_ML2, X = X, y = y, lambda = .01, type = &#39;L1&#39;, control = list(reltol = 1e-12) )$par round( rbind( glmnet_L2 = t(as.matrix(coef(glmnetL2))), pars_L2 = pars_L2, glmnet_L1 = t(as.matrix(coef(glmnetL1))), pars_L1 = pars_L1 ), digits = 4 ) (Intercept) V1 V2 s0 -0.4324 0.1301 0.1094 pars_L2 -0.4324 0.1301 0.1094 s0 -0.4325 0.1207 0.1005 pars_L1 -0.4325 0.1207 0.1005 Source Base R source code found at https://github.com/m-clark/Miscellaneous-R-Code/blob/master/ModelFitting/penalized_ML.R "],["lasso.html", "L1 (lasso) regularization Functions Data setup Comparison Source", " L1 (lasso) regularization See Tibshirani (1996) for the source, or Murphy PML (2012) for a nice overview (watch for typos in depictions). A more conceptual depiction of the lasso can be found in penalized_ML.R. Functions Coordinate descent lasso &lt;- function( X, # model matrix y, # target lambda = .1, # penalty parameter soft = TRUE, # soft vs. hard thresholding tol = 1e-6, # tolerance iter = 100, # number of max iterations verbose = TRUE # print out iteration number ) { # soft thresholding function soft_thresh &lt;- function(a, b) { out = rep(0, length(a)) out[a &gt; b] = a[a &gt; b] - b out[a &lt; -b] = a[a &lt; -b] + b out } w = solve(crossprod(X) + diag(lambda, ncol(X))) %*% crossprod(X,y) tol_curr = 1 J = ncol(X) a = rep(0, J) c_ = rep(0, J) i = 1 while (tol &lt; tol_curr &amp;&amp; i &lt; iter) { w_old = w a = colSums(X^2) l = length(y)*lambda # for consistency with glmnet approach c_ = sapply(1:J, function(j) sum( X[,j] * (y - X[,-j] %*% w_old[-j]) )) if (soft) { for (j in 1:J) { w[j] = soft_thresh(c_[j]/a[j], l/a[j]) } } else { w = w_old w[c_&lt; l &amp; c_ &gt; -l] = 0 } tol_curr = crossprod(w - w_old) i = i + 1 if (verbose &amp;&amp; i%%10 == 0) message(i) } w } Data setup set.seed(8675309) N = 500 p = 10 X = scale(matrix(rnorm(N*p), ncol=p)) b = c(.5, -.5, .25, -.25, .125, -.125, rep(0, p-6)) y = scale(X %*% b + rnorm(N, sd=.5)) lambda = .1 Note, if lambda=0, result is the same as lm.fit. result_soft = lasso( X, y, lambda = lambda, tol = 1e-12, soft = TRUE ) result_hard = lasso( X, y, lambda = lambda, tol = 1e-12, soft = FALSE ) glmnet is by default a mixture of ridge and lasso penalties, setting alpha = 1 reduces to lasso (alpha=0 would be ridge). We set the lambda to a couple values while only wanting the one set to the same lambda value as above (s). library(glmnet) glmnet_res = coef( glmnet( X, y, alpha = 1, lambda = c(10, 1, lambda), thresh = 1e-12, intercept = FALSE ), s = lambda ) library(lassoshooting) ls_res = lassoshooting( X = X, y = y, lambda = length(y) * lambda, thr = 1e-12 ) Comparison data.frame( lm = coef(lm(y ~ . - 1, data.frame(X))), lasso_soft = result_soft, lasso_hard = result_hard, lspack = ls_res$coef, glmnet = glmnet_res[-1, 1], truth = b ) lm lasso_soft lasso_hard lspack glmnet truth X1 0.534988063 0.43542527 0.5348784 0.43542528 0.43552489 0.500 X2 -0.529993422 -0.42876539 -0.5298847 -0.42876538 -0.42886718 -0.500 X3 0.234376590 0.12436834 0.2343207 0.12436835 0.12447920 0.250 X4 -0.294350608 -0.20743074 -0.2942946 -0.20743075 -0.20751883 -0.250 X5 0.126037566 0.02036410 0.1260132 0.02036407 0.02047015 0.125 X6 -0.159386728 -0.05501971 -0.1593572 -0.05501969 -0.05512364 -0.125 X7 -0.016718534 0.00000000 0.0000000 0.00000000 0.00000000 0.000 X8 0.009894575 0.00000000 0.0000000 0.00000000 0.00000000 0.000 X9 -0.005441959 0.00000000 0.0000000 0.00000000 0.00000000 0.000 X10 0.010561128 0.00000000 0.0000000 0.00000000 0.00000000 0.000 Source Base R source code found at https://github.com/m-clark/Miscellaneous-R-Code/blob/master/ModelFitting/lasso.R "],["ridge.html", "L2 (ridge) regularization Comparison Source", " L2 (ridge) regularization Compare to lasso chapter. A more conceptual depiction of the lasso can be found in the penalized ML chapter. ridge &lt;- function(w, X, y, lambda = .1) { # X: model matrix; # y: target; # lambda: penalty parameter; # w: the weights/coefficients crossprod(y - X %*% w) + lambda * length(y) * crossprod(w) } set.seed(8675309) N = 500 p = 10 X = scale(matrix(rnorm(N * p), ncol = p)) b = c(.5, -.5, .25, -.25, .125, -.125, rep(0, 4)) y = scale(X %*% b + rnorm(N, sd = .5)) Note, if lambda = 0, result is the same as lm.fit. result_ridge = optim( rep(0, ncol(X)), ridge, X = X, y = y, lambda = .1, method = &#39;BFGS&#39; ) Analytical result. result_ridge2 = solve(crossprod(X) + diag(length(y)*.1, ncol(X))) %*% crossprod(X, y) Alternative with augmented data (note sigma is ignored as it equals 1, but otherwise X/sigma and y/sigma). X2 = rbind(X, diag(sqrt(length(y)*.1), ncol(X))) y2 = c(y, rep(0, ncol(X))) result_ridge3 = solve(crossprod(X2)) %*% crossprod(X2, y2) glmnet is by default a mixture of ridge and lasso penalties, setting alpha = 1 reduces to lasso, while alpha=0 would be ridge. library(glmnet) glmnet_res = coef( glmnet( X, y, alpha = 0, lambda = c(10, 1, .1), thresh = 1e-12, intercept = F ), s = .1 ) Comparison data.frame( lm = coef(lm(y ~ . - 1, data.frame(X))), ridge = result_ridge$par, ridge2 = result_ridge2, ridge3 = result_ridge3, glmnet = glmnet_res[-1, 1], truth = b ) lm ridge ridge2 ridge3 glmnet truth X1 0.534988063 0.485323748 0.485323748 0.485323748 0.485368766 0.500 X2 -0.529993422 -0.480742032 -0.480742032 -0.480742032 -0.480786661 -0.500 X3 0.234376590 0.209412833 0.209412833 0.209412833 0.209435147 0.250 X4 -0.294350608 -0.268814168 -0.268814168 -0.268814168 -0.268837476 -0.250 X5 0.126037566 0.114963716 0.114963716 0.114963716 0.114973801 0.125 X6 -0.159386728 -0.145880488 -0.145880488 -0.145880488 -0.145892837 -0.125 X7 -0.016718534 -0.021658889 -0.021658889 -0.021658889 -0.021655033 0.000 X8 0.009894575 0.006956965 0.006956965 0.006956965 0.006959470 0.000 X9 -0.005441959 0.001392244 0.001392244 0.001392244 0.001386661 0.000 X10 0.010561128 0.010985385 0.010985385 0.010985385 0.010985102 0.000 Source Base R source code found at https://github.com/m-clark/Miscellaneous-R-Code/blob/master/ModelFitting/ridge.R "],["newton-irls.html", "Newton and IRLS GLM estimation examples Data Prep Functions Comparison Source", " Newton and IRLS GLM estimation examples Examples of maximum likelihood estimation via a variety of means. See the gradientdescent.R script for that approach. Here we demonstrate Newton’s and Iterated Reweighted Least Squares approaches via logistic regression. For the following, I had Murphy’s PML text open and more or less followed the algorithms in chapter 8. Note that for Newton’s method, this doesn’t implement a line search to find a more optimal stepsize at a given iteration. Data Prep Predict graduate school admission based on gre, gpa, and school rank (higher=more prestige). See corresponding demo here: https://stats.idre.ucla.edu/stata/dae/logistic-regression/. The only difference is that I treat rank as numeric rather than categorical. admit = haven::read_dta(&#39;https://stats.idre.ucla.edu/stat/stata/dae/binary.dta&#39;) comparison_model = glm(admit ~ gre + gpa + rank, data = admit, family = binomial) summary(comparison_model) Call: glm(formula = admit ~ gre + gpa + rank, family = binomial, data = admit) Deviance Residuals: Min 1Q Median 3Q Max -1.5802 -0.8848 -0.6382 1.1575 2.1732 Coefficients: Estimate Std. Error z value Pr(&gt;|z|) (Intercept) -3.449549 1.132846 -3.045 0.00233 ** gre 0.002294 0.001092 2.101 0.03564 * gpa 0.777014 0.327484 2.373 0.01766 * rank -0.560031 0.127137 -4.405 1.06e-05 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 (Dispersion parameter for binomial family taken to be 1) Null deviance: 499.98 on 399 degrees of freedom Residual deviance: 459.44 on 396 degrees of freedom AIC: 467.44 Number of Fisher Scoring iterations: 4 X = model.matrix(comparison_model) y = comparison_model$y Functions Newton’s method newton &lt;- function( X, y, tol = 1e-12, iter = 500, stepsize = .5 ) { # Args: # X: model matrix # y: target # tol: tolerance # iter: maximum number of iterations # stepsize: (0, 1) # intialize int = log(mean(y) / (1 - mean(y))) # intercept beta = c(int, rep(0, ncol(X) - 1)) currtol = 1 it = 0 ll = 0 while (currtol &gt; tol &amp;&amp; it &lt; iter) { it = it +1 ll_old = ll mu = plogis(X %*% beta)[,1] g = crossprod(X, mu-y) # gradient S = diag(mu*(1-mu)) H = t(X) %*% S %*% X # hessian beta = beta - stepsize * solve(H) %*% g ll = sum(dbinom(y, prob = mu, size = 1, log = TRUE)) currtol = abs(ll - ll_old) } list( beta = beta, iter = it, tol = currtol, loglik = ll ) } Compare to base R glm. newton_result = newton( X = X, y = y, stepsize = .9, tol = 1e-8 # tol set to 1e-8 as in glm default ) newton_result $beta [,1] (Intercept) -3.449548577 gre 0.002293959 gpa 0.777013649 rank -0.560031371 $iter [1] 8 $tol [1] 2.581544e-10 $loglik [1] -229.7209 comparison_model Call: glm(formula = admit ~ gre + gpa + rank, family = binomial, data = admit) Coefficients: (Intercept) gre gpa rank -3.449549 0.002294 0.777014 -0.560031 Degrees of Freedom: 399 Total (i.e. Null); 396 Residual Null Deviance: 500 Residual Deviance: 459.4 AIC: 467.4 rbind( newton = unlist(newton_result), glm_default = c( beta = coef(comparison_model), comparison_model$iter, tol = NA, loglik = -logLik(comparison_model) ) ) beta1 beta2 beta3 beta4 iter tol loglik newton -3.449549 0.002293959 0.7770136 -0.5600314 8 2.581544e-10 -229.7209 glm_default -3.449549 0.002293959 0.7770137 -0.5600314 4 NA 229.7209 IRLS Note that glm is actually using IRLS, so the results from this should be fairly spot on. irls &lt;- function(X, y, tol = 1e-12, iter = 500) { # intialize int = log(mean(y) / (1 - mean(y))) # intercept beta = c(int, rep(0, ncol(X) - 1)) currtol = 1 it = 0 ll = 0 while (currtol &gt; tol &amp;&amp; it &lt; iter) { it = it + 1 ll_old = ll eta = X %*% beta mu = plogis(eta)[,1] s = mu * (1 - mu) S = diag(s) z = eta + (y-mu)/s beta = solve(t(X) %*% S %*% X) %*% (t(X) %*% (S %*% z)) ll = sum( dbinom( y, prob = plogis(X %*% beta), size = 1, log = T ) ) currtol = abs(ll - ll_old) } list( beta = beta, iter = it, tol = currtol, loglik = ll, weights = plogis(X %*% beta) * (1 - plogis(X %*% beta)) ) } tol set to 1e-8 as in glm default. irls_result = irls(X = X, y = y, tol = 1e-8) str(irls_result) List of 5 $ beta : num [1:4, 1] -3.44955 0.00229 0.77701 -0.56003 ..- attr(*, &quot;dimnames&quot;)=List of 2 .. ..$ : chr [1:4] &quot;(Intercept)&quot; &quot;gre&quot; &quot;gpa&quot; &quot;rank&quot; .. ..$ : NULL $ iter : num 4 $ tol : num 6e-09 $ loglik : num -230 $ weights: num [1:400, 1] 0.1536 0.2168 0.2026 0.1268 0.0884 ... ..- attr(*, &quot;dimnames&quot;)=List of 2 .. ..$ : chr [1:400] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; ... .. ..$ : NULL comparison_model Call: glm(formula = admit ~ gre + gpa + rank, family = binomial, data = admit) Coefficients: (Intercept) gre gpa rank -3.449549 0.002294 0.777014 -0.560031 Degrees of Freedom: 399 Total (i.e. Null); 396 Residual Null Deviance: 500 Residual Deviance: 459.4 AIC: 467.4 Comparison Compare all results. rbind( newton = unlist(newton_result), irls = unlist(irls_result[-length(irls_result)]), glm_default = c( beta = coef(comparison_model), comparison_model$iter, tol = NA, loglik = logLik(comparison_model) ) ) beta1 beta2 beta3 beta4 iter tol loglik newton -3.449549 0.002293959 0.7770136 -0.5600314 8 2.581544e-10 -229.7209 irls -3.449549 0.002293959 0.7770137 -0.5600314 4 5.996583e-09 -229.7209 glm_default -3.449549 0.002293959 0.7770137 -0.5600314 4 NA -229.7209 Compare weights. head(cbind(irls_result$weights, comparison_model$weights)) [,1] [,2] 1 0.15362250 0.15362250 2 0.21679615 0.21679615 3 0.20255723 0.20255724 4 0.12676333 0.12676334 5 0.08835918 0.08835918 6 0.23528108 0.23528108 Source Base R source code found at https://github.com/m-clark/Miscellaneous-R-Code/blob/master/ModelFitting/newton_irls.R "],["nelder-mead.html", "Nelder Mead First version Second version Source", " Nelder Mead This is based on the pure Python implementation by François Chollet found at https://github.com/fchollet/nelder-mead (also in the Miscellaneous R code repo at nelder_mead.py). This is mostly just an academic exercise on my part. I’m not sure how much one would use the basic NM for many situations. In my experience BFGS and other approaches would be faster, more accurate, and less sensitive to starting values for the types of problems I’ve played around with. Others who actually spend their time researching such things seem to agree. There were two issues on (GitHub)[https://github.com/fchollet/nelder-mead/issues/2] regarding the original code, and I’ve implemented the suggested corrections with notes. The initial function code is not very R-like, as the goal was to keep more similar to the original Python for comparison, which used a list approach. I also provide a more R-like/cleaner version that uses matrices instead of lists, but which still sticks the same approach for the most part. For both functions, comparisons are made using the optimx package, but feel free to use base R’s optim instead. f function to optimize, must return a scalar score and operate over an array of the same dimensions as x_start x_start initial position step look-around radius in initial step no_improve_thr See no_improv_break no_improv_break break after no_improv_break iterations with an improvement lower than no_improv_thr max_iter always break after this number of iterations. Set it to 0 to loop indefinitely. alpha parameters of the algorithm (see Wikipedia page for reference) gamma parameters of the algorithm (see Wikipedia page for reference) rho parameters of the algorithm (see Wikipedia page for reference) sigma parameters of the algorithm (see Wikipedia page for reference) verbose Print iterations? This function returns the best parameter array and best score. First version nelder_mead = function( f, x_start, step = 0.1, no_improve_thr = 1e-12, no_improv_break = 10, max_iter = 0, alpha = 1, gamma = 2, rho = 0.5, sigma = 0.5, verbose = FALSE ) { # init dim = length(x_start) prev_best = f(x_start) no_improv = 0 res = list(list(x_start = x_start, prev_best = prev_best)) for (i in 1:dim) { x = x_start x[i] = x[i] + step score = f(x) res = append(res, list(list(x_start = x, prev_best = score))) } # simplex iter iters = 0 while (TRUE) { # order idx = sapply(res, `[[`, 2) res = res[order(idx)] # ascending order best = res[[1]][[2]] # break after max_iter if (max_iter &gt; 0 &amp; iters &gt;= max_iter) return(res[[1]]) iters = iters + 1 # break after no_improv_break iterations with no improvement if (verbose) message(paste(&#39;...best so far:&#39;, best)) if (best &lt; (prev_best - no_improve_thr)) { no_improv = 0 prev_best = best } else { no_improv = no_improv + 1 } if (no_improv &gt;= no_improv_break) return(res[[1]]) # centroid x0 = rep(0, dim) for (tup in 1:(length(res)-1)) { for (i in 1:dim) { x0[i] = x0[i] + res[[tup]][[1]][i] / (length(res)-1) } } # reflection xr = x0 + alpha * (x0 - res[[length(res)]][[1]]) rscore = f(xr) if (res[[1]][[2]] &lt;= rscore &amp; rscore &lt; res[[length(res)-1]][[2]]) { res[[length(res)]] = list(xr, rscore) next } # expansion if (rscore &lt; res[[1]][[2]]) { # xe = x0 + gamma*(x0 - res[[length(res)]][[1]]) # issue with this xe = x0 + gamma * (xr - x0) escore = f(xe) if (escore &lt; rscore) { res[[length(res)]] = list(xe, escore) next } else { res[[length(res)]] = list(xr, rscore) next } } # contraction # xc = x0 + rho*(x0 - res[[length(res)]][[1]]) # issue with wiki consistency for rho values (and optim) xc = x0 + rho * (res[[length(res)]][[1]] - x0) cscore = f(xc) if (cscore &lt; res[[length(res)]][[2]]) { res[[length(res)]] = list(xc, cscore) next } # reduction x1 = res[[1]][[1]] nres = list() for (tup in res) { redx = x1 + sigma * (tup[[1]] - x1) score = f(redx) nres = append(nres, list(list(redx, score))) } res = nres } } Example The function to minimize. f = function(x) { sin(x[1]) * cos(x[2]) * (1 / (abs(x[3]) + 1)) } Estimate. nelder_mead( f, c(0, 0, 0), max_iter = 1000, no_improve_thr = 1e-12 ) [[1]] [1] -1.570797e+00 -2.235577e-07 1.637460e-14 [[2]] [1] -1 Compare to optimx. You may see warnings. optimx::optimx( par = c(0, 0, 0), fn = f, method = &quot;Nelder-Mead&quot;, control = list( alpha = 1, gamma = 2, beta = 0.5, maxit = 1000, reltol = 1e-12 ) ) p1 p2 p3 value fevals gevals niter convcode kkt1 kkt2 xtime Nelder-Mead -1.570796 1.394018e-08 1.088215e-16 -1 861 NA NA 0 TRUE TRUE 0.001 A Regression Model I find a regression model to be more applicable/intuitive for my needs, so provide an example for that case. Data setup set.seed(8675309) N = 500 npreds = 5 X = cbind(1, matrix(rnorm(N * npreds), ncol = npreds)) beta = runif(ncol(X), -1, 1) y = X %*% beta + rnorm(nrow(X)) Least squares loss function to minimize. f = function(b) { crossprod(y - X %*% b)[,1] # if using optimx need scalar } lm estimates. lm.fit(X, y)$coef x1 x2 x3 x4 x5 x6 -0.96214657 0.59432481 0.04864576 0.27573466 0.97525840 -0.07470287 nm_result = nelder_mead( f, runif(ncol(X)), max_iter = 2000, no_improve_thr = 1e-12, verbose = FALSE ) Comparison Compare to optimx. opt_out = optimx::optimx( runif(ncol(X)), fn = f, # model function method = &#39;Nelder-Mead&#39;, control = list( alpha = 1, gamma = 2, beta = 0.5, #rho maxit = 2000, reltol = 1e-12 ) ) rbind( nm_func = unlist(nm_result), nm_optimx = opt_out[1:7] ) p1 p2 p3 p4 p5 p6 value nm_func -0.9621510 0.594327 0.04864183 0.2757265 0.9752524 -0.07470389 501.3155 nm_optimx -0.9621494 0.594325 0.04864620 0.2757383 0.9752579 -0.07470054 501.3155 Second version This is a more natural R approach in my opinion. nelder_mead2 = function( f, x_start, step = 0.1, no_improve_thr = 1e-12, no_improv_break = 10, max_iter = 0, alpha = 1, gamma = 2, rho = 0.5, sigma = 0.5, verbose = FALSE ) { # init npar = length(x_start) nc = npar + 1 prev_best = f(x_start) no_improv = 0 res = matrix(c(x_start, prev_best), ncol = nc) colnames(res) = c(paste(&#39;par&#39;, 1:npar, sep = &#39;_&#39;), &#39;score&#39;) for (i in 1:npar) { x = x_start x[i] = x[i] + step score = f(x) res = rbind(res, c(x, score)) } # simplex iter iters = 0 while (TRUE) { # order res = res[order(res[, nc]), ] # ascending order best = res[1, nc] # break after max_iter if (max_iter &amp; iters &gt;= max_iter) return(res[1, ]) iters = iters + 1 # break after no_improv_break iterations with no improvement if (verbose) message(paste(&#39;...best so far:&#39;, best)) if (best &lt; (prev_best - no_improve_thr)) { no_improv = 0 prev_best = best } else { no_improv = no_improv + 1 } if (no_improv &gt;= no_improv_break) return(res[1, ]) nr = nrow(res) # centroid: more efficient than previous double loop x0 = colMeans(res[(1:npar), -nc]) # reflection xr = x0 + alpha * (x0 - res[nr, -nc]) rscore = f(xr) if (res[1, &#39;score&#39;] &lt;= rscore &amp; rscore &lt; res[npar, &#39;score&#39;]) { res[nr,] = c(xr, rscore) next } # expansion if (rscore &lt; res[1, &#39;score&#39;]) { xe = x0 + gamma * (xr - x0) escore = f(xe) if (escore &lt; rscore) { res[nr, ] = c(xe, escore) next } else { res[nr, ] = c(xr, rscore) next } } # contraction xc = x0 + rho * (res[nr, -nc] - x0) cscore = f(xc) if (cscore &lt; res[nr, &#39;score&#39;]) { res[nr,] = c(xc, cscore) next } # reduction x1 = res[1, -nc] nres = res for (i in 1:nr) { redx = x1 + sigma * (res[i, -nc] - x1) score = f(redx) nres[i, ] = c(redx, score) } res = nres } } Example function f = function(x) { sin(x[1]) * cos(x[2]) * (1 / (abs(x[3]) + 1)) } nelder_mead2( f, c(0, 0, 0), max_iter = 1000, no_improve_thr = 1e-12 ) par_1 par_2 par_3 score -1.570797e+00 -2.235577e-07 1.622809e-14 -1.000000e+00 optimx::optimx( par = c(0, 0, 0), fn = f, method = &quot;Nelder-Mead&quot;, control = list( alpha = 1, gamma = 2, beta = 0.5, maxit = 1000, reltol = 1e-12 ) ) p1 p2 p3 value fevals gevals niter convcode kkt1 kkt2 xtime Nelder-Mead -1.570796 1.394018e-08 1.088215e-16 -1 861 NA NA 0 TRUE TRUE 0.001 A Regression Model set.seed(8675309) N = 500 npreds = 5 X = cbind(1, matrix(rnorm(N * npreds), ncol = npreds)) beta = runif(ncol(X), -1, 1) y = X %*% beta + rnorm(nrow(X)) Least squares loss function to minimize. f = function(b) { crossprod(y - X %*% b)[,1] # if using optimx need scalar } lm_par = lm.fit(X, y)$coef nm_par = nelder_mead2( f, runif(ncol(X)), max_iter = 2000, no_improve_thr = 1e-12 ) Comparison Compare to optimx. opt_par = optimx::optimx( runif(ncol(X)), fn = f, method = &#39;Nelder-Mead&#39;, control = list( alpha = 1, gamma = 2, beta = 0.5, maxit = 2000, reltol = 1e-12 ) )[1:(npreds + 1)] rbind( lm = lm_par, nm = nm_par, optimx = opt_par, truth = beta ) p1 p2 p3 p4 p5 p6 lm -0.9621466 0.5943248 0.04864576 0.2757347 0.9752584 -0.07470287 nm -0.9621510 0.5943270 0.04864183 0.2757265 0.9752524 -0.07470389 optimx -0.9621494 0.5943250 0.04864620 0.2757383 0.9752579 -0.07470054 truth -0.9087584 0.6195267 0.07358131 0.3196977 0.9561050 -0.07977885 Source Base R source code found at https://github.com/m-clark/Miscellaneous-R-Code/blob/master/ModelFitting/nelder_mead.R "],["gradient-descent.html", "Gradient Descent Data Setup Gradient Descent Algorithm Source", " Gradient Descent Gradient descent for a standard linear regression model. The function takes arguments starting points for the parameters to be estimated, a tolerance or maximum iteration value to provide a stopping point, stepsize (or starting stepsize for adaptive approach), whether to print out iterations, and whether to plot the loss over each iteration. Data Setup Create some basic data for standard regression. set.seed(8675309) n = 1000 x1 = rnorm(n) x2 = rnorm(n) y = 1 + .5*x1 + .2*x2 + rnorm(n) X = cbind(Intercept = 1, x1, x2) # model matrix Gradient Descent Algorithm gd = function( par, X, y, tolerance = 1e-3, maxit = 1000, stepsize = 1e-3, adapt = FALSE, verbose = TRUE, plotLoss = TRUE ) { # initialize beta = par; names(beta) = colnames(X) loss = crossprod(X %*% beta - y) tol = 1 iter = 1 while(tol &gt; tolerance &amp;&amp; iter &lt; maxit){ LP = X %*% beta grad = t(X) %*% (LP - y) betaCurrent = beta - stepsize * grad tol = max(abs(betaCurrent - beta)) beta = betaCurrent loss = append(loss, crossprod(LP - y)) iter = iter + 1 if (adapt) stepsize = ifelse( loss[iter] &lt; loss[iter - 1], stepsize * 1.2, stepsize * .8 ) if (verbose &amp;&amp; iter %% 10 == 0) message(paste(&#39;Iteration:&#39;, iter)) } if (plotLoss) plot(loss, type = &#39;l&#39;, bty = &#39;n&#39;) list( par = beta, loss = loss, RSE = sqrt(crossprod(LP - y) / (nrow(X) - ncol(X))), iter = iter, fitted = LP ) } Run Set starting values. init = rep(0, 3) For any particular data you’d have to fiddle with the stepsize, which could be assessed via cross-validation, or alternatively one can use an adaptive approach, a simple one of which is implemented in this function. gd_result = gd( init, X = X, y = y, tolerance = 1e-8, stepsize = 1e-4, adapt = TRUE ) str(gd_result) List of 5 $ par : num [1:3, 1] 0.985 0.487 0.218 ..- attr(*, &quot;dimnames&quot;)=List of 2 .. ..$ : chr [1:3] &quot;Intercept&quot; &quot;x1&quot; &quot;x2&quot; .. ..$ : NULL $ loss : num [1:70] 2315 2315 2075 1918 1760 ... $ RSE : num [1, 1] 1.03 $ iter : num 70 $ fitted: num [1:1000, 1] 0.441 1.061 0.43 2.125 1.858 ... Comparison We can compare to standard linear regression. rbind( gd = round(gd_result$par[, 1], 5), lm = coef(lm(y ~ x1 + x2)) ) Intercept x1 x2 gd 0.9847800 0.4867900 0.2175200 lm 0.9847803 0.4867896 0.2175169 # summary(lm(y ~ x1 + x2)) Source Base R source code found at https://github.com/m-clark/Miscellaneous-R-Code/blob/master/ModelFitting/gradient_descent.R "],["stochastic-gradient-descent.html", "Stochastic Gradient Descent Data Setup Stochastic Gradient Descent Algorithm Data Set Shift Source", " Stochastic Gradient Descent Here we have ‘online’ learning via stochastic gradient descent. See also, standard gradient descent In the following, we have basic data for standard regression, but in this ‘online’ learning case, we can assume each observation comes to us as a stream over time rather than as a single batch, and would continue coming in. Note that there are plenty of variations of this, and it can be applied in the batch case as well. Currently no stopping point is implemented in order to trace results over all data points/iterations. On revisiting this much later, I thought it useful to add that I believe this was motivated by the example in Murphy’s Probabilistic Machine Learning. I also made some cleanup to my original code, added some comments, but mostly left it as it was. Data Setup set.seed(1234) n = 1000 x1 = rnorm(n) x2 = rnorm(n) y = 1 + .5*x1 + .2*x2 + rnorm(n) X = cbind(Intercept = 1, x1, x2) Stochastic Gradient Descent Algorithm sgd = function( par, # parameter estimates X, # model matrix y, # target variable stepsize = 1, # the learning rate stepsizeTau = 0, # if &gt; 0, a check on the LR at early iterations average = FALSE ){ # initialize beta = par names(beta) = colnames(X) betamat = matrix(0, nrow(X), ncol = length(beta)) # Collect all estimates fits = NA # fitted values s = 0 # adagrad per parameter learning rate adjustment loss = NA # Collect loss at each point for (i in 1:nrow(X)) { Xi = X[i, , drop = FALSE] yi = y[i] LP = Xi %*% beta # matrix operations not necessary, grad = t(Xi) %*% (LP - yi) # but makes consistent with the standard gd R file s = s + grad^2 beta = beta - stepsize * grad/(stepsizeTau + sqrt(s)) # adagrad approach if (average &amp; i &gt; 1) { beta = beta - 1/i * (betamat[i - 1, ] - beta) # a variation } betamat[i,] = beta fits[i] = LP loss[i] = (LP - yi)^2 } LP = X %*% beta lastloss = crossprod(LP - y) list( par = beta, # final estimates parvec = betamat, # all estimates loss = loss, # observation level loss RMSE = sqrt(sum(lastloss)/nrow(X)), fitted = fits ) } Run Set starting values. init = rep(0, 3) For any particular data you might have to fiddle with the stepsize, perhaps choosing one based on cross-validation with old data. sgd_result = sgd( init, X = X, y = y, stepsize = .1, stepsizeTau = .5, average = FALSE ) str(sgd_result) List of 5 $ par : num [1:3, 1] 1.024 0.537 0.148 ..- attr(*, &quot;dimnames&quot;)=List of 2 .. ..$ : chr [1:3] &quot;Intercept&quot; &quot;x1&quot; &quot;x2&quot; .. ..$ : NULL $ parvec: num [1:1000, 1:3] -0.06208 -0.00264 0.04781 0.09866 0.08242 ... $ loss : num [1:1000] 0.67 1.261 1.365 2.043 0.215 ... $ RMSE : num 1.01 $ fitted: num [1:1000] 0 -0.0236 -0.0446 -0.2828 0.1634 ... sgd_result$par [,1] Intercept 1.0241049 x1 0.5368198 x2 0.1478470 Comparison We can compare to standard linear regression. # summary(lm(y ~ x1 + x2)) coef1 = coef(lm(y ~ x1 + x2)) rbind( sgd_result = sgd_result$par[, 1], lm = coef1 ) Intercept x1 x2 sgd_result 1.024105 0.5368198 0.1478470 lm 1.029957 0.5177020 0.1631026 Visualize Estimates library(tidyverse) gd = data.frame(sgd_result$parvec) %&gt;% mutate(Iteration = 1:n()) gd = gd %&gt;% pivot_longer(cols = -Iteration, names_to = &#39;Parameter&#39;, values_to = &#39;Value&#39;) %&gt;% mutate(Parameter = factor(Parameter, labels = colnames(X))) ggplot(aes( x = Iteration, y = Value, group = Parameter, color = Parameter ), data = gd) + geom_path() + geom_point(data = filter(gd, Iteration == n), size = 3) + geom_text( aes(label = round(Value, 2)), hjust = -.5, angle = 45, size = 4, data = filter(gd, Iteration == n) ) + theme_minimal() Data Set Shift This data includes a shift of the previous data. set.seed(1234) n2 = 1000 x1.2 = rnorm(n2) x2.2 = rnorm(n2) y2 = -1 + .25*x1.2 - .25*x2.2 + rnorm(n2) X2 = rbind(X, cbind(1, x1.2, x2.2)) coef2 = coef(lm(y2 ~ x1.2 + x2.2)) y2 = c(y, y2) n3 = 1000 x1.3 = rnorm(n3) x2.3 = rnorm(n3) y3 = 1 - .25*x1.3 + .25*x2.3 + rnorm(n3) coef3 = coef(lm(y3 ~ x1.3 + x2.3)) X3 = rbind(X2, cbind(1, x1.3, x2.3)) y3 = c(y2, y3) Run sgd_result2 = sgd( init, X = X3, y = y3, stepsize = 1, stepsizeTau = 0, average = FALSE ) str(sgd_result2) List of 5 $ par : num [1:3, 1] 0.821 -0.223 0.211 ..- attr(*, &quot;dimnames&quot;)=List of 2 .. ..$ : chr [1:3] &quot;Intercept&quot; &quot;x1&quot; &quot;x2&quot; .. ..$ : NULL $ parvec: num [1:3000, 1:3] -1 -0.119 0.624 1.531 1.063 ... $ loss : num [1:3000] 0.67 2.31 3.69 30.99 10.58 ... $ RMSE : num 1.57 $ fitted: num [1:3000] 0 -0.421 -0.797 -4.421 2.952 ... Comparison Compare with lm for each data part. sgd_result2$parvec[c(n, n + n2, n + n2 + n3), ] [,1] [,2] [,3] [1,] 1.0859378 0.5128904 0.1457697 [2,] -0.9246994 0.2945723 -0.2941759 [3,] 0.8213521 -0.2229918 0.2112883 rbind(coef1, coef2, coef3) (Intercept) x1 x2 coef1 1.0299573 0.5177020 0.1631026 coef2 -0.9700427 0.2677020 -0.2868974 coef3 1.0453166 -0.2358521 0.2418489 Visualize Estimates Visualize estimates. gd = data.frame(sgd_result2$parvec) %&gt;% mutate(Iteration = 1:n()) gd = gd %&gt;% pivot_longer(cols = -Iteration, names_to = &#39;Parameter&#39;, values_to = &#39;Value&#39;) %&gt;% mutate(Parameter = factor(Parameter, labels = colnames(X))) ggplot(aes(x = Iteration, y = Value, group = Parameter, color = Parameter ), data = gd) + geom_path() + geom_point(data = filter(gd, Iteration %in% c(n, n + n2, n + n2 + n3)), size = 3) + geom_text( aes(label = round(Value, 2)), hjust = -.5, angle = 45, data = filter(gd, Iteration %in% c(n, n + n2, n + n2 + n3)), size = 4, show.legend = FALSE ) + theme_minimal() Source Base R source code found at https://github.com/m-clark/Miscellaneous-R-Code/blob/master/ModelFitting/stochastic_gradient_descent.R "],["supplemental.html", "Supplemental Other Languages", " Supplemental Other Languages When doing some of these models and algorithms, I had some other code to work with in another language, or, at the time, just wanted to try it in that language. Not much here but may be useful to some. Julia One-factor Mixed Model Two-factor Mixed Model Matlab One-factor Mixed Model Two-factor Mixed Model Algorithms Python Nelder-Mead HMM "]]
