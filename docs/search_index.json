[["index.html", "Models by Example Demonstrations with R and friends.", " Models by Example Demonstrations with R and friends. Michael Clark m-clark.github.io 2020-11-01 "],["introduction.html", "Introduction", " Introduction This document provides ‘by-hand’ demonstrations of various models and algorithms. The goal is to take away some of the mystery by providing clean code examples that are easy to run and compare with other tools. The code is not meant to be extensive, or used in production. Almost everything here has a package/module that would do it far better and efficiently. The document itself is also not an introduction to any of these methods, and in fact contains very little expository text. This document is merely a learning tool for those wanting to dive a little deeper. The source code for these demonstrations may be found at their original home here: https://github.com/m-clark/Miscellaneous-R-Code While I’m happy to fix any glaring errors, this is pretty much a completed document, except on the off chance I add to it. Perhaps if others would like to add to it via pull requests, I would do so. "],["linear-regression.html", "Standard Linear Regression Data Setup Functions Obtain Model Estimates Comparison Source", " Standard Linear Regression A standard regression model via maximum likelihood or least squares loss. Also examples for QR decomposition and normal equations. Can serve as an entry point for those starting out to the wider world of computational statistics as maximum likelihood is the fundamental approach used in most applied statistics, but which is also a key aspect of the Bayesian approach. Least squares loss is not confined to the standard lm setting, but is widely used in more predictive/‘algorithmic’ approaches e.g. in machine learning and elsewhere. Data Setup set.seed(123) # ensures replication # predictors and response N = 100 # sample size k = 2 # number of desired predictors X = matrix(rnorm(N * k), ncol = k) y = -.5 + .2*X[, 1] + .1*X[, 2] + rnorm(N, sd = .5) # increasing N will get estimated values closer to these dfXy = data.frame(X, y) Functions A maximum likelihood approach. lm_ML = function(par, X, y) { # par: parameters to be estimated # X: predictor matrix with intercept column # y: response # setup beta = par[-1] # coefficients sigma2 = par[1] # error variance sigma = sqrt(sigma2) N = nrow(X) # linear predictor LP = X %*% beta # linear predictor mu = LP # identity link in the glm sense # calculate likelihood L = dnorm(y, mean = mu, sd = sigma, log = TRUE) # log likelihood # L = -.5*N*log(sigma2) - .5*(1/sigma2)*crossprod(y-mu) # alternate log likelihood form -sum(L) # optim by default is minimization, and we want to maximize the likelihood # (see also fnscale in optim.control) } An approach via least squares loss function. lm_LS = function(par, X, y) { # arguments- # par: parameters to be estimated # X: predictor matrix with intercept column # y: response # setup beta = par # coefficients # linear predictor LP = X %*% beta # linear predictor mu = LP # identity link # calculate least squares loss function L = crossprod(y - mu) } Obtain Model Estimates Setup for use with optim. X = cbind(1, X) Initial values. Note we’d normally want to handle the sigma differently as it’s bounded by zero, but we’ll ignore for demonstration. Also sigma2 is not required for the LS approach as it is the objective function. init = c(1, rep(0, ncol(X))) names(init) = c(&#39;sigma2&#39;, &#39;intercept&#39;, &#39;b1&#39;, &#39;b2&#39;) optlmML = optim( par = init, fn = lm_ML, X = X, y = y, control = list(reltol = 1e-8) ) optlmLS = optim( par = init[-1], fn = lm_LS, X = X, y = y, control = list(reltol = 1e-8) ) pars_ML = optlmML$par pars_LS = c(sigma2 = optlmLS$value / (N - k - 1), optlmLS$par) # calculate sigma2 and add Comparison Compare to lm which uses QR decomposition. modlm = lm(y ~ ., dfXy) Example of QR. # QRX = qr(X) # Q = qr.Q(QRX) # R = qr.R(QRX) # Bhat = solve(R) %*% crossprod(Q, y) # alternate: qr.coef(QRX, y) round( rbind( pars_ML, pars_LS, modlm = c(summary(modlm)$sigma^2, coef(modlm))), digits = 3 ) sigma2 intercept b1 b2 pars_ML 0.219 -0.432 0.133 0.112 pars_LS 0.226 -0.432 0.133 0.112 modlm 0.226 -0.432 0.133 0.112 The slight difference in sigma is roughly maxlike dividing by N vs. N-k-1 in the traditional least squares approach; diminishes with increasing N as both tend toward whatever sd^2 you specify when creating the y response above. Compare to glm, which by default assumes gaussian family with identity link and uses lm.fit. modglm = glm(y ~ ., data = dfXy) summary(modglm) Call: glm(formula = y ~ ., data = dfXy) Deviance Residuals: Min 1Q Median 3Q Max -0.93651 -0.33037 -0.06222 0.31068 1.03991 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) -0.43247 0.04807 -8.997 1.97e-14 *** X1 0.13341 0.05243 2.544 0.0125 * X2 0.11191 0.04950 2.261 0.0260 * --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 (Dispersion parameter for gaussian family taken to be 0.2262419) Null deviance: 24.444 on 99 degrees of freedom Residual deviance: 21.945 on 97 degrees of freedom AIC: 140.13 Number of Fisher Scoring iterations: 2 Via normal equations. coefs = solve(t(X) %*% X) %*% t(X) %*% y # coefficients Compare. sqrt(crossprod(y - X %*% coefs) / (N - k - 1)) [,1] [1,] 0.4756489 summary(modlm)$sigma [1] 0.4756489 sqrt(modglm$deviance / modglm$df.residual) [1] 0.4756489 c(sqrt(pars_ML[1]), sqrt(pars_LS[1])) sigma2 sigma2 0.4684616 0.4756490 # rerun by adding 3-4 zeros to the N Source Base R source code found at https://github.com/m-clark/Miscellaneous-R-Code/blob/master/ModelFitting/standard_lm.R "],["logistic-regression.html", "Standard Logistic Rodels Data Setup Functions Obtain Model Estimates Comparison Source", " Standard Logistic Rodels A standard logistic regression model via maximum likelihood or exponential loss. Can serve as an entry point for those starting out to the wider world of computational statistics as maximum likelihood is the fundamental approach used in most applied statistics, but which is also a key aspect of the Bayesian approach. Exponential loss is not confined to the standard glm setting, but is widely used in more predictive/‘algorithmic’ approaches e.g. in machine learning and elsewhere. This follows the linear regression model approach. Data Setup Predictors and target. set.seed(1235) # ensures replication N = 10000 # sample size k = 2 # number of desired predictors X = matrix(rnorm(N * k), ncol = k) # the linear predictor lp = -.5 + .2 * X[, 1] + .1 * X[, 2] # increasing N will get estimated values closer to these y = rbinom(N, size = 1, prob = plogis(lp)) dfXy = data.frame(X, y) Functions A maximum likelihood approach. logreg_ML = function(par, X, y) { # arguments- # par: parameters to be estimated # X: predictor matrix with intercept column # y: response # setup beta = par # coefficients N = nrow(X) # linear predictor LP = X %*% beta # linear predictor mu = plogis(LP) # logit link # calculate likelihood L = dbinom(y, size = 1, prob = mu, log = TRUE) # log likelihood # L = y*log(mu) + (1 - y)*log(1-mu) # alternate log likelihood form -sum(L) # optim by default is minimization, and we want to maximize the likelihood # (see also fnscale in optim.control) } Another approach via exponential loss function. logreg_exp = function(par, X, y) { # arguments- # par: parameters to be estimated # X: predictor matrix with intercept column # y: response # setup beta = par # coefficients # linear predictor LP = X %*% beta # linear predictor # calculate exponential loss function (convert y to -1:1 from 0:1) L = sum(exp(-ifelse(y, 1, -1) * .5 * LP)) } Obtain Model Estimates Setup for use with optim. X = cbind(1, X) # initial values init = rep(0, ncol(X)) names(init) = c(&#39;intercept&#39;, &#39;b1&#39;, &#39;b2&#39;) optlmML = optim( par = init, fn = logreg_ML, X = X, y = y, control = list(reltol = 1e-8) ) optglmClass = optim( par = init, fn = logreg_exp, X = X, y = y, control = list(reltol = 1e-15) ) pars_ML = optlmML$par pars_exp = optglmClass$par Comparison Compare to glm. modglm = glm(y ~ ., dfXy, family = binomial) rbind( pars_ML, pars_exp, pars_GLM = coef(modglm) ) intercept b1 b2 pars_ML -0.5117658 0.2378927 0.08019841 pars_exp -0.5114284 0.2368478 0.07907056 pars_GLM -0.5117321 0.2378743 0.08032617 Source Base R source code found at https://github.com/m-clark/Miscellaneous-R-Code/blob/master/ModelFitting/standard_logistic.R "],["penalized-maximum-likelihood.html", "Penalized Maximum Likelihood Data Setup Functions Obtain Model Estimates Comparison Source", " Penalized Maximum Likelihood A standard regression model via penalized likelihood. See the standard_lm.R code for comparison. Here the penalty is specified (via lambda argument) but one would typically estimate via cross-validation or some other fashion. Two penalties are possible with the function. One using the (squared) L2 norm (aka ridge regression, tikhonov regularization), another using the L1 norm (aka lasso) which has the possibility of penalizing coefficients to zero, and thus can serve as a model selection procedure. I have a more technical appraoch to the lasso in the lasso.R file. Note that both L2 and L1 approaches can be seen as maximum a posteriori (MAP) estimates for a Bayesian regression with a specific prior on the coefficients. The L2 approach is akin to a normal prior with zero mean, while L1 is akin to a zero mean Laplace prior. See the Bayesian scripts for ways to implement. Data Setup set.seed(123) # ensures replication # predictors and response N = 100 # sample size k = 2 # number of desired predictors X = matrix(rnorm(N * k), ncol = k) y = -.5 + .2*X[, 1] + .1*X[, 2] + rnorm(N, sd = .5) # increasing N will get estimated values closer to these dfXy = data.frame(X,y) Functions A maximum likelihood approach. penalized_ML = function(par, X, y, lambda = .1, type = &#39;L2&#39;) { # arguments- # par: parameters to be estimated # X: predictor matrix with intercept column # y: response # lambda: penalty coefficient # type: penalty approach # setup beta = par[-1] # coefficients sigma2 = par[1] # error variance sigma = sqrt(sigma2) N = nrow(X) # linear predictor LP = X %*% beta # linear predictor mu = LP # identity link in the glm sense # calculate likelihood L = dnorm(y, mean = mu, sd = sigma, log = T) # log likelihood PL = switch(type, &#39;L2&#39; = -sum(L) + lambda * crossprod(beta[-1]), # the intercept is not penalized &#39;L1&#39; = -sum(L) + lambda * sum(abs(beta[-1])) ) } glmnet style approach that will put the lambda coefficient on equivalent scale. Uses a different objective function. Note that glmnet is actually elasticnet and mixes both L1 and L2 penalties. penalized_ML2 = function(par, X, y, lambda = .1, type = &#39;L2&#39;) { # arguments- # par: parameters to be estimated # X: predictor matrix with intercept column # y: response # lambda: penalty coefficient # type: penalty approach # setup beta = par # coefficients N = nrow(X) # linear predictor LP = X %*% beta # linear predictor mu = LP # identity link in the glm sense obj = switch(type, &#39;L2&#39; = .5*crossprod(y - X %*% beta)/N + lambda * crossprod(beta[-1]), &#39;L1&#39; = .5*crossprod(y - X %*% beta)/N + lambda * sum(abs(beta[-1])) ) } Obtain Model Estimates Setup the model matrix for use with optim. X = cbind(1, X) Initial values. note we’d normally want to handle the sigma differently as it’s bounded by zero, but we’ll ignore for demonstration. Also sigma2 is not required for the LS approach. init = c(1, rep(0, ncol(X))) names(init) = c(&#39;sigma2&#39;, &#39;intercept&#39;, &#39;b1&#39;, &#39;b2&#39;) optlmpenalized_MLL2 = optim( par = init, fn = penalized_ML, X = X, y = y, lambda = 1, control = list(reltol = 1e-12) ) optlmpenalized_MLL1 = optim( par = init, fn = penalized_ML, X = X, y = y, lambda = 1, type = &#39;L1&#39;, control = list(reltol = 1e-12) ) parspenalized_MLL2 = optlmpenalized_MLL2$par parspenalized_MLL1 = optlmpenalized_MLL1$par Comparison Compare to lm. modlm = lm(y ~ ., dfXy) round( rbind( parspenalized_MLL2, parspenalized_MLL1, modlm = c(summary(modlm)$sigma ^ 2, coef(modlm)) ), digits = 4 ) sigma2 intercept b1 b2 parspenalized_MLL2 0.2195 -0.4325 0.1327 0.1113 parspenalized_MLL1 0.2195 -0.4325 0.1306 0.1094 modlm 0.2262 -0.4325 0.1334 0.1119 Compare to glmnet. Setting alpha to 0 and 1 is equivalent to L2 and L1 penalties respectively. You also wouldn’t want to specify lambda normally, and rather let it come about as part of the estimation procedure. We do so here just for demonstration. library(glmnet) glmnetL2 = glmnet( X[, -1], y, alpha = 0, lambda = .01, standardize = FALSE ) glmnetL1 = glmnet( X[, -1], y, alpha = 1, lambda = .01, standardize = FALSE ) pars_L2 = optim( par = init[-1], fn = penalized_ML2, X = X, y = y, lambda = .01, control = list(reltol = 1e-12) )$par pars_L1 = optim( par = init[-1], fn = penalized_ML2, X = X, y = y, lambda = .01, type = &#39;L1&#39;, control = list(reltol = 1e-12) )$par round( rbind( glmnet_L2 = t(as.matrix(coef(glmnetL2))), pars_L2 = pars_L2, glmnet_L1 = t(as.matrix(coef(glmnetL1))), pars_L1 = pars_L1 ), digits = 4 ) (Intercept) V1 V2 s0 -0.4324 0.1301 0.1094 pars_L2 -0.4324 0.1301 0.1094 s0 -0.4325 0.1207 0.1005 pars_L1 -0.4325 0.1207 0.1005 Source Base R source code found at https://github.com/m-clark/Miscellaneous-R-Code/blob/master/ModelFitting/penalized_ML.R "],["lasso.html", "L1 (lasso) regularization Functions Data setup Comparison Source", " L1 (lasso) regularization See Tibshirani (1996) for the source, or Murphy PML (2012) for a nice overview (watch for typos in depictions). A more conceptual depiction of the lasso can be found in penalized_ML.R. Functions Coordinate descent lasso &lt;- function( X, # model matrix y, # target lambda = .1, # penalty parameter soft = TRUE, # soft vs. hard thresholding tol = 1e-6, # tolerance iter = 100, # number of max iterations verbose = TRUE # print out iteration number ) { # soft thresholding function soft_thresh &lt;- function(a, b) { out = rep(0, length(a)) out[a &gt; b] = a[a &gt; b] - b out[a &lt; -b] = a[a &lt; -b] + b out } w = solve(crossprod(X) + diag(lambda, ncol(X))) %*% crossprod(X,y) tol_curr = 1 J = ncol(X) a = rep(0, J) c_ = rep(0, J) i = 1 while (tol &lt; tol_curr &amp;&amp; i &lt; iter) { w_old = w a = colSums(X^2) l = length(y)*lambda # for consistency with glmnet approach c_ = sapply(1:J, function(j) sum( X[,j] * (y - X[,-j] %*% w_old[-j]) )) if (soft) { for (j in 1:J) { w[j] = soft_thresh(c_[j]/a[j], l/a[j]) } } else { w = w_old w[c_&lt; l &amp; c_ &gt; -l] = 0 } tol_curr = crossprod(w - w_old) i = i + 1 if (verbose &amp;&amp; i%%10 == 0) message(i) } w } Data setup set.seed(8675309) N = 500 p = 10 X = scale(matrix(rnorm(N*p), ncol=p)) b = c(.5, -.5, .25, -.25, .125, -.125, rep(0, p-6)) y = scale(X %*% b + rnorm(N, sd=.5)) lambda = .1 Note, if lambda=0, result is the same as lm.fit. result_soft = lasso( X, y, lambda = lambda, tol = 1e-12, soft = TRUE ) result_hard = lasso( X, y, lambda = lambda, tol = 1e-12, soft = FALSE ) glmnet is by default a mixture of ridge and lasso penalties, setting alpha = 1 reduces to lasso (alpha=0 would be ridge). We set the lambda to a couple values while only wanting the one set to the same lambda value as above (s). library(glmnet) glmnet_res = coef( glmnet( X, y, alpha = 1, lambda = c(10, 1, lambda), thresh = 1e-12, intercept = FALSE ), s = lambda ) library(lassoshooting) ls_res = lassoshooting( X = X, y = y, lambda = length(y) * lambda, thr = 1e-12 ) Comparison data.frame( lm = coef(lm(y ~ . - 1, data.frame(X))), lasso_soft = result_soft, lasso_hard = result_hard, lspack = ls_res$coef, glmnet = glmnet_res[-1, 1], truth = b ) lm lasso_soft lasso_hard lspack glmnet truth X1 0.534988063 0.43542527 0.5348784 0.43542528 0.43552489 0.500 X2 -0.529993422 -0.42876539 -0.5298847 -0.42876538 -0.42886718 -0.500 X3 0.234376590 0.12436834 0.2343207 0.12436835 0.12447920 0.250 X4 -0.294350608 -0.20743074 -0.2942946 -0.20743075 -0.20751883 -0.250 X5 0.126037566 0.02036410 0.1260132 0.02036407 0.02047015 0.125 X6 -0.159386728 -0.05501971 -0.1593572 -0.05501969 -0.05512364 -0.125 X7 -0.016718534 0.00000000 0.0000000 0.00000000 0.00000000 0.000 X8 0.009894575 0.00000000 0.0000000 0.00000000 0.00000000 0.000 X9 -0.005441959 0.00000000 0.0000000 0.00000000 0.00000000 0.000 X10 0.010561128 0.00000000 0.0000000 0.00000000 0.00000000 0.000 Source Base R source code found at https://github.com/m-clark/Miscellaneous-R-Code/blob/master/ModelFitting/lasso.R "],["ridge.html", "L2 (ridge) regularization Comparison Source", " L2 (ridge) regularization Compare to lasso chapter. A more conceptual depiction of the lasso can be found in the penalized ML chapter. ridge &lt;- function(w, X, y, lambda = .1) { # X: model matrix; # y: target; # lambda: penalty parameter; # w: the weights/coefficients crossprod(y - X %*% w) + lambda * length(y) * crossprod(w) } set.seed(8675309) N = 500 p = 10 X = scale(matrix(rnorm(N * p), ncol = p)) b = c(.5, -.5, .25, -.25, .125, -.125, rep(0, 4)) y = scale(X %*% b + rnorm(N, sd = .5)) Note, if lambda = 0, result is the same as lm.fit. result_ridge = optim( rep(0, ncol(X)), ridge, X = X, y = y, lambda = .1, method = &#39;BFGS&#39; ) Analytical result. result_ridge2 = solve(crossprod(X) + diag(length(y)*.1, ncol(X))) %*% crossprod(X, y) Alternative with augmented data (note sigma is ignored as it equals 1, but otherwise X/sigma and y/sigma). X2 = rbind(X, diag(sqrt(length(y)*.1), ncol(X))) y2 = c(y, rep(0, ncol(X))) result_ridge3 = solve(crossprod(X2)) %*% crossprod(X2, y2) glmnet is by default a mixture of ridge and lasso penalties, setting alpha = 1 reduces to lasso, while alpha=0 would be ridge. library(glmnet) glmnet_res = coef( glmnet( X, y, alpha = 0, lambda = c(10, 1, .1), thresh = 1e-12, intercept = F ), s = .1 ) Comparison data.frame( lm = coef(lm(y ~ . - 1, data.frame(X))), ridge = result_ridge$par, ridge2 = result_ridge2, ridge3 = result_ridge3, glmnet = glmnet_res[-1, 1], truth = b ) lm ridge ridge2 ridge3 glmnet truth X1 0.534988063 0.485323748 0.485323748 0.485323748 0.485368766 0.500 X2 -0.529993422 -0.480742032 -0.480742032 -0.480742032 -0.480786661 -0.500 X3 0.234376590 0.209412833 0.209412833 0.209412833 0.209435147 0.250 X4 -0.294350608 -0.268814168 -0.268814168 -0.268814168 -0.268837476 -0.250 X5 0.126037566 0.114963716 0.114963716 0.114963716 0.114973801 0.125 X6 -0.159386728 -0.145880488 -0.145880488 -0.145880488 -0.145892837 -0.125 X7 -0.016718534 -0.021658889 -0.021658889 -0.021658889 -0.021655033 0.000 X8 0.009894575 0.006956965 0.006956965 0.006956965 0.006959470 0.000 X9 -0.005441959 0.001392244 0.001392244 0.001392244 0.001386661 0.000 X10 0.010561128 0.010985385 0.010985385 0.010985385 0.010985102 0.000 Source Base R source code found at https://github.com/m-clark/Miscellaneous-R-Code/blob/master/ModelFitting/ridge.R "],["newton-irls.html", "Newton and IRLS GLM estimation examples Data Prep Functions Comparison Source", " Newton and IRLS GLM estimation examples Examples of maximum likelihood estimation via a variety of means. See the gradientdescent.R script for that approach. Here we demonstrate Newton’s and Iterated Reweighted Least Squares approaches via logistic regression. For the following, I had Murphy’s PML text open and more or less followed the algorithms in chapter 8. Note that for Newton’s method, this doesn’t implement a line search to find a more optimal stepsize at a given iteration. Data Prep Predict graduate school admission based on gre, gpa, and school rank (higher=more prestige). See corresponding demo here: https://stats.idre.ucla.edu/stata/dae/logistic-regression/. The only difference is that I treat rank as numeric rather than categorical. admit = haven::read_dta(&#39;https://stats.idre.ucla.edu/stat/stata/dae/binary.dta&#39;) comparison_model = glm(admit ~ gre + gpa + rank, data = admit, family = binomial) summary(comparison_model) Call: glm(formula = admit ~ gre + gpa + rank, family = binomial, data = admit) Deviance Residuals: Min 1Q Median 3Q Max -1.5802 -0.8848 -0.6382 1.1575 2.1732 Coefficients: Estimate Std. Error z value Pr(&gt;|z|) (Intercept) -3.449549 1.132846 -3.045 0.00233 ** gre 0.002294 0.001092 2.101 0.03564 * gpa 0.777014 0.327484 2.373 0.01766 * rank -0.560031 0.127137 -4.405 1.06e-05 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 (Dispersion parameter for binomial family taken to be 1) Null deviance: 499.98 on 399 degrees of freedom Residual deviance: 459.44 on 396 degrees of freedom AIC: 467.44 Number of Fisher Scoring iterations: 4 X = model.matrix(comparison_model) y = comparison_model$y Functions Newton’s method newton &lt;- function( X, y, tol = 1e-12, iter = 500, stepsize = .5 ) { # Args: # X: model matrix # y: target # tol: tolerance # iter: maximum number of iterations # stepsize: (0, 1) # intialize int = log(mean(y) / (1 - mean(y))) # intercept beta = c(int, rep(0, ncol(X) - 1)) currtol = 1 it = 0 ll = 0 while (currtol &gt; tol &amp;&amp; it &lt; iter) { it = it +1 ll_old = ll mu = plogis(X %*% beta)[,1] g = crossprod(X, mu-y) # gradient S = diag(mu*(1-mu)) H = t(X) %*% S %*% X # hessian beta = beta - stepsize * solve(H) %*% g ll = sum(dbinom(y, prob = mu, size = 1, log = TRUE)) currtol = abs(ll - ll_old) } list( beta = beta, iter = it, tol = currtol, loglik = ll ) } Compare to base R glm. newton_result = newton( X = X, y = y, stepsize = .9, tol = 1e-8 # tol set to 1e-8 as in glm default ) newton_result $beta [,1] (Intercept) -3.449548577 gre 0.002293959 gpa 0.777013649 rank -0.560031371 $iter [1] 8 $tol [1] 2.581544e-10 $loglik [1] -229.7209 comparison_model Call: glm(formula = admit ~ gre + gpa + rank, family = binomial, data = admit) Coefficients: (Intercept) gre gpa rank -3.449549 0.002294 0.777014 -0.560031 Degrees of Freedom: 399 Total (i.e. Null); 396 Residual Null Deviance: 500 Residual Deviance: 459.4 AIC: 467.4 rbind( newton = unlist(newton_result), glm_default = c( beta = coef(comparison_model), comparison_model$iter, tol = NA, loglik = -logLik(comparison_model) ) ) beta1 beta2 beta3 beta4 iter tol loglik newton -3.449549 0.002293959 0.7770136 -0.5600314 8 2.581544e-10 -229.7209 glm_default -3.449549 0.002293959 0.7770137 -0.5600314 4 NA 229.7209 IRLS Note that glm is actually using IRLS, so the results from this should be fairly spot on. irls &lt;- function(X, y, tol = 1e-12, iter = 500) { # intialize int = log(mean(y) / (1 - mean(y))) # intercept beta = c(int, rep(0, ncol(X) - 1)) currtol = 1 it = 0 ll = 0 while (currtol &gt; tol &amp;&amp; it &lt; iter) { it = it + 1 ll_old = ll eta = X %*% beta mu = plogis(eta)[,1] s = mu * (1 - mu) S = diag(s) z = eta + (y-mu)/s beta = solve(t(X) %*% S %*% X) %*% (t(X) %*% (S %*% z)) ll = sum( dbinom( y, prob = plogis(X %*% beta), size = 1, log = T ) ) currtol = abs(ll - ll_old) } list( beta = beta, iter = it, tol = currtol, loglik = ll, weights = plogis(X %*% beta) * (1 - plogis(X %*% beta)) ) } tol set to 1e-8 as in glm default. irls_result = irls(X = X, y = y, tol = 1e-8) str(irls_result) List of 5 $ beta : num [1:4, 1] -3.44955 0.00229 0.77701 -0.56003 ..- attr(*, &quot;dimnames&quot;)=List of 2 .. ..$ : chr [1:4] &quot;(Intercept)&quot; &quot;gre&quot; &quot;gpa&quot; &quot;rank&quot; .. ..$ : NULL $ iter : num 4 $ tol : num 6e-09 $ loglik : num -230 $ weights: num [1:400, 1] 0.1536 0.2168 0.2026 0.1268 0.0884 ... ..- attr(*, &quot;dimnames&quot;)=List of 2 .. ..$ : chr [1:400] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; ... .. ..$ : NULL comparison_model Call: glm(formula = admit ~ gre + gpa + rank, family = binomial, data = admit) Coefficients: (Intercept) gre gpa rank -3.449549 0.002294 0.777014 -0.560031 Degrees of Freedom: 399 Total (i.e. Null); 396 Residual Null Deviance: 500 Residual Deviance: 459.4 AIC: 467.4 Comparison Compare all results. rbind( newton = unlist(newton_result), irls = unlist(irls_result[-length(irls_result)]), glm_default = c( beta = coef(comparison_model), comparison_model$iter, tol = NA, loglik = logLik(comparison_model) ) ) beta1 beta2 beta3 beta4 iter tol loglik newton -3.449549 0.002293959 0.7770136 -0.5600314 8 2.581544e-10 -229.7209 irls -3.449549 0.002293959 0.7770137 -0.5600314 4 5.996583e-09 -229.7209 glm_default -3.449549 0.002293959 0.7770137 -0.5600314 4 NA -229.7209 Compare weights. head(cbind(irls_result$weights, comparison_model$weights)) [,1] [,2] 1 0.15362250 0.15362250 2 0.21679615 0.21679615 3 0.20255723 0.20255724 4 0.12676333 0.12676334 5 0.08835918 0.08835918 6 0.23528108 0.23528108 Source Base R source code found at https://github.com/m-clark/Miscellaneous-R-Code/blob/master/ModelFitting/newton_irls.R "],["nelder-mead.html", "Nelder Mead First version Second version Source", " Nelder Mead This is based on the pure Python implementation by François Chollet found at https://github.com/fchollet/nelder-mead (also in the Miscellaneous R code repo at nelder_mead.py). This is mostly just an academic exercise on my part. I’m not sure how much one would use the basic NM for many situations. In my experience BFGS and other approaches would be faster, more accurate, and less sensitive to starting values for the types of problems I’ve played around with. Others who actually spend their time researching such things seem to agree. There were two issues on (GitHub)[https://github.com/fchollet/nelder-mead/issues/2] regarding the original code, and I’ve implemented the suggested corrections with notes. The initial function code is not very R-like, as the goal was to keep more similar to the original Python for comparison, which used a list approach. I also provide a more R-like/cleaner version that uses matrices instead of lists, but which still sticks the same approach for the most part. For both functions, comparisons are made using the optimx package, but feel free to use base R’s optim instead. f function to optimize, must return a scalar score and operate over an array of the same dimensions as x_start x_start initial position step look-around radius in initial step no_improve_thr See no_improv_break no_improv_break break after no_improv_break iterations with an improvement lower than no_improv_thr max_iter always break after this number of iterations. Set it to 0 to loop indefinitely. alpha parameters of the algorithm (see Wikipedia page for reference) gamma parameters of the algorithm (see Wikipedia page for reference) rho parameters of the algorithm (see Wikipedia page for reference) sigma parameters of the algorithm (see Wikipedia page for reference) verbose Print iterations? This function returns the best parameter array and best score. First version nelder_mead = function( f, x_start, step = 0.1, no_improve_thr = 1e-12, no_improv_break = 10, max_iter = 0, alpha = 1, gamma = 2, rho = 0.5, sigma = 0.5, verbose = FALSE ) { # init dim = length(x_start) prev_best = f(x_start) no_improv = 0 res = list(list(x_start = x_start, prev_best = prev_best)) for (i in 1:dim) { x = x_start x[i] = x[i] + step score = f(x) res = append(res, list(list(x_start = x, prev_best = score))) } # simplex iter iters = 0 while (TRUE) { # order idx = sapply(res, `[[`, 2) res = res[order(idx)] # ascending order best = res[[1]][[2]] # break after max_iter if (max_iter &gt; 0 &amp; iters &gt;= max_iter) return(res[[1]]) iters = iters + 1 # break after no_improv_break iterations with no improvement if (verbose) message(paste(&#39;...best so far:&#39;, best)) if (best &lt; (prev_best - no_improve_thr)) { no_improv = 0 prev_best = best } else { no_improv = no_improv + 1 } if (no_improv &gt;= no_improv_break) return(res[[1]]) # centroid x0 = rep(0, dim) for (tup in 1:(length(res)-1)) { for (i in 1:dim) { x0[i] = x0[i] + res[[tup]][[1]][i] / (length(res)-1) } } # reflection xr = x0 + alpha * (x0 - res[[length(res)]][[1]]) rscore = f(xr) if (res[[1]][[2]] &lt;= rscore &amp; rscore &lt; res[[length(res)-1]][[2]]) { res[[length(res)]] = list(xr, rscore) next } # expansion if (rscore &lt; res[[1]][[2]]) { # xe = x0 + gamma*(x0 - res[[length(res)]][[1]]) # issue with this xe = x0 + gamma * (xr - x0) escore = f(xe) if (escore &lt; rscore) { res[[length(res)]] = list(xe, escore) next } else { res[[length(res)]] = list(xr, rscore) next } } # contraction # xc = x0 + rho*(x0 - res[[length(res)]][[1]]) # issue with wiki consistency for rho values (and optim) xc = x0 + rho * (res[[length(res)]][[1]] - x0) cscore = f(xc) if (cscore &lt; res[[length(res)]][[2]]) { res[[length(res)]] = list(xc, cscore) next } # reduction x1 = res[[1]][[1]] nres = list() for (tup in res) { redx = x1 + sigma * (tup[[1]] - x1) score = f(redx) nres = append(nres, list(list(redx, score))) } res = nres } } Example The function to minimize. f = function(x) { sin(x[1]) * cos(x[2]) * (1 / (abs(x[3]) + 1)) } Estimate. nelder_mead( f, c(0, 0, 0), max_iter = 1000, no_improve_thr = 1e-12 ) [[1]] [1] -1.570797e+00 -2.235577e-07 1.637460e-14 [[2]] [1] -1 Compare to optimx. You may see warnings. optimx::optimx( par = c(0, 0, 0), fn = f, method = &quot;Nelder-Mead&quot;, control = list( alpha = 1, gamma = 2, beta = 0.5, maxit = 1000, reltol = 1e-12 ) ) p1 p2 p3 value fevals gevals niter convcode kkt1 kkt2 xtime Nelder-Mead -1.570796 1.394018e-08 1.088215e-16 -1 861 NA NA 0 TRUE TRUE 0.001 A Regression Model I find a regression model to be more applicable/intuitive for my needs, so provide an example for that case. Data setup set.seed(8675309) N = 500 npreds = 5 X = cbind(1, matrix(rnorm(N * npreds), ncol = npreds)) beta = runif(ncol(X), -1, 1) y = X %*% beta + rnorm(nrow(X)) Least squares loss function to minimize. f = function(b) { crossprod(y - X %*% b)[,1] # if using optimx need scalar } lm estimates. lm.fit(X, y)$coef x1 x2 x3 x4 x5 x6 -0.96214657 0.59432481 0.04864576 0.27573466 0.97525840 -0.07470287 nm_result = nelder_mead( f, runif(ncol(X)), max_iter = 2000, no_improve_thr = 1e-12, verbose = FALSE ) Comparison Compare to optimx. opt_out = optimx::optimx( runif(ncol(X)), fn = f, # model function method = &#39;Nelder-Mead&#39;, control = list( alpha = 1, gamma = 2, beta = 0.5, #rho maxit = 2000, reltol = 1e-12 ) ) rbind( nm_func = unlist(nm_result), nm_optimx = opt_out[1:7] ) p1 p2 p3 p4 p5 p6 value nm_func -0.9621510 0.594327 0.04864183 0.2757265 0.9752524 -0.07470389 501.3155 nm_optimx -0.9621494 0.594325 0.04864620 0.2757383 0.9752579 -0.07470054 501.3155 Second version This is a more natural R approach in my opinion. nelder_mead2 = function( f, x_start, step = 0.1, no_improve_thr = 1e-12, no_improv_break = 10, max_iter = 0, alpha = 1, gamma = 2, rho = 0.5, sigma = 0.5, verbose = FALSE ) { # init npar = length(x_start) nc = npar + 1 prev_best = f(x_start) no_improv = 0 res = matrix(c(x_start, prev_best), ncol = nc) colnames(res) = c(paste(&#39;par&#39;, 1:npar, sep = &#39;_&#39;), &#39;score&#39;) for (i in 1:npar) { x = x_start x[i] = x[i] + step score = f(x) res = rbind(res, c(x, score)) } # simplex iter iters = 0 while (TRUE) { # order res = res[order(res[, nc]), ] # ascending order best = res[1, nc] # break after max_iter if (max_iter &amp; iters &gt;= max_iter) return(res[1, ]) iters = iters + 1 # break after no_improv_break iterations with no improvement if (verbose) message(paste(&#39;...best so far:&#39;, best)) if (best &lt; (prev_best - no_improve_thr)) { no_improv = 0 prev_best = best } else { no_improv = no_improv + 1 } if (no_improv &gt;= no_improv_break) return(res[1, ]) nr = nrow(res) # centroid: more efficient than previous double loop x0 = colMeans(res[(1:npar), -nc]) # reflection xr = x0 + alpha * (x0 - res[nr, -nc]) rscore = f(xr) if (res[1, &#39;score&#39;] &lt;= rscore &amp; rscore &lt; res[npar, &#39;score&#39;]) { res[nr,] = c(xr, rscore) next } # expansion if (rscore &lt; res[1, &#39;score&#39;]) { xe = x0 + gamma * (xr - x0) escore = f(xe) if (escore &lt; rscore) { res[nr, ] = c(xe, escore) next } else { res[nr, ] = c(xr, rscore) next } } # contraction xc = x0 + rho * (res[nr, -nc] - x0) cscore = f(xc) if (cscore &lt; res[nr, &#39;score&#39;]) { res[nr,] = c(xc, cscore) next } # reduction x1 = res[1, -nc] nres = res for (i in 1:nr) { redx = x1 + sigma * (res[i, -nc] - x1) score = f(redx) nres[i, ] = c(redx, score) } res = nres } } Example function f = function(x) { sin(x[1]) * cos(x[2]) * (1 / (abs(x[3]) + 1)) } nelder_mead2( f, c(0, 0, 0), max_iter = 1000, no_improve_thr = 1e-12 ) par_1 par_2 par_3 score -1.570797e+00 -2.235577e-07 1.622809e-14 -1.000000e+00 optimx::optimx( par = c(0, 0, 0), fn = f, method = &quot;Nelder-Mead&quot;, control = list( alpha = 1, gamma = 2, beta = 0.5, maxit = 1000, reltol = 1e-12 ) ) p1 p2 p3 value fevals gevals niter convcode kkt1 kkt2 xtime Nelder-Mead -1.570796 1.394018e-08 1.088215e-16 -1 861 NA NA 0 TRUE TRUE 0.001 A Regression Model set.seed(8675309) N = 500 npreds = 5 X = cbind(1, matrix(rnorm(N * npreds), ncol = npreds)) beta = runif(ncol(X), -1, 1) y = X %*% beta + rnorm(nrow(X)) Least squares loss function to minimize. f = function(b) { crossprod(y - X %*% b)[,1] # if using optimx need scalar } lm_par = lm.fit(X, y)$coef nm_par = nelder_mead2( f, runif(ncol(X)), max_iter = 2000, no_improve_thr = 1e-12 ) Comparison Compare to optimx. opt_par = optimx::optimx( runif(ncol(X)), fn = f, method = &#39;Nelder-Mead&#39;, control = list( alpha = 1, gamma = 2, beta = 0.5, maxit = 2000, reltol = 1e-12 ) )[1:(npreds + 1)] rbind( lm = lm_par, nm = nm_par, optimx = opt_par, truth = beta ) p1 p2 p3 p4 p5 p6 lm -0.9621466 0.5943248 0.04864576 0.2757347 0.9752584 -0.07470287 nm -0.9621510 0.5943270 0.04864183 0.2757265 0.9752524 -0.07470389 optimx -0.9621494 0.5943250 0.04864620 0.2757383 0.9752579 -0.07470054 truth -0.9087584 0.6195267 0.07358131 0.3196977 0.9561050 -0.07977885 Source Base R source code found at https://github.com/m-clark/Miscellaneous-R-Code/blob/master/ModelFitting/nelder_mead.R "],["gradient-descent.html", "Gradient Descent Data Setup Gradient Descent Algorithm Source", " Gradient Descent Gradient descent for a standard linear regression model. The function takes arguments starting points for the parameters to be estimated, a tolerance or maximum iteration value to provide a stopping point, stepsize (or starting stepsize for adaptive approach), whether to print out iterations, and whether to plot the loss over each iteration. Data Setup Create some basic data for standard regression. set.seed(8675309) n = 1000 x1 = rnorm(n) x2 = rnorm(n) y = 1 + .5*x1 + .2*x2 + rnorm(n) X = cbind(Intercept = 1, x1, x2) # model matrix Gradient Descent Algorithm gd = function( par, X, y, tolerance = 1e-3, maxit = 1000, stepsize = 1e-3, adapt = FALSE, verbose = TRUE, plotLoss = TRUE ) { # initialize beta = par; names(beta) = colnames(X) loss = crossprod(X %*% beta - y) tol = 1 iter = 1 while(tol &gt; tolerance &amp;&amp; iter &lt; maxit){ LP = X %*% beta grad = t(X) %*% (LP - y) betaCurrent = beta - stepsize * grad tol = max(abs(betaCurrent - beta)) beta = betaCurrent loss = append(loss, crossprod(LP - y)) iter = iter + 1 if (adapt) stepsize = ifelse( loss[iter] &lt; loss[iter - 1], stepsize * 1.2, stepsize * .8 ) if (verbose &amp;&amp; iter %% 10 == 0) message(paste(&#39;Iteration:&#39;, iter)) } if (plotLoss) plot(loss, type = &#39;l&#39;, bty = &#39;n&#39;) list( par = beta, loss = loss, RSE = sqrt(crossprod(LP - y) / (nrow(X) - ncol(X))), iter = iter, fitted = LP ) } Run Set starting values. init = rep(0, 3) For any particular data you’d have to fiddle with the stepsize, which could be assessed via cross-validation, or alternatively one can use an adaptive approach, a simple one of which is implemented in this function. gd_result = gd( init, X = X, y = y, tolerance = 1e-8, stepsize = 1e-4, adapt = TRUE ) str(gd_result) List of 5 $ par : num [1:3, 1] 0.985 0.487 0.218 ..- attr(*, &quot;dimnames&quot;)=List of 2 .. ..$ : chr [1:3] &quot;Intercept&quot; &quot;x1&quot; &quot;x2&quot; .. ..$ : NULL $ loss : num [1:70] 2315 2315 2075 1918 1760 ... $ RSE : num [1, 1] 1.03 $ iter : num 70 $ fitted: num [1:1000, 1] 0.441 1.061 0.43 2.125 1.858 ... Comparison We can compare to standard linear regression. rbind( gd = round(gd_result$par[, 1], 5), lm = coef(lm(y ~ x1 + x2)) ) Intercept x1 x2 gd 0.9847800 0.4867900 0.2175200 lm 0.9847803 0.4867896 0.2175169 # summary(lm(y ~ x1 + x2)) Source Base R source code found at https://github.com/m-clark/Miscellaneous-R-Code/blob/master/ModelFitting/gradient_descent.R "],["stochastic-gradient-descent.html", "Stochastic Gradient Descent Data Setup Stochastic Gradient Descent Algorithm Data Set Shift Source", " Stochastic Gradient Descent Here we have ‘online’ learning via stochastic gradient descent. See also, standard gradient descent In the following, we have basic data for standard regression, but in this ‘online’ learning case, we can assume each observation comes to us as a stream over time rather than as a single batch, and would continue coming in. Note that there are plenty of variations of this, and it can be applied in the batch case as well. Currently no stopping point is implemented in order to trace results over all data points/iterations. On revisiting this much later, I thought it useful to add that I believe this was motivated by the example in Murphy’s Probabilistic Machine Learning. I also made some cleanup to my original code, added some comments, but mostly left it as it was. Data Setup set.seed(1234) n = 1000 x1 = rnorm(n) x2 = rnorm(n) y = 1 + .5*x1 + .2*x2 + rnorm(n) X = cbind(Intercept = 1, x1, x2) Stochastic Gradient Descent Algorithm sgd = function( par, # parameter estimates X, # model matrix y, # target variable stepsize = 1, # the learning rate stepsizeTau = 0, # if &gt; 0, a check on the LR at early iterations average = FALSE ){ # initialize beta = par names(beta) = colnames(X) betamat = matrix(0, nrow(X), ncol = length(beta)) # Collect all estimates fits = NA # fitted values s = 0 # adagrad per parameter learning rate adjustment loss = NA # Collect loss at each point for (i in 1:nrow(X)) { Xi = X[i, , drop = FALSE] yi = y[i] LP = Xi %*% beta # matrix operations not necessary, grad = t(Xi) %*% (LP - yi) # but makes consistent with the standard gd R file s = s + grad^2 beta = beta - stepsize * grad/(stepsizeTau + sqrt(s)) # adagrad approach if (average &amp; i &gt; 1) { beta = beta - 1/i * (betamat[i - 1, ] - beta) # a variation } betamat[i,] = beta fits[i] = LP loss[i] = (LP - yi)^2 } LP = X %*% beta lastloss = crossprod(LP - y) list( par = beta, # final estimates parvec = betamat, # all estimates loss = loss, # observation level loss RMSE = sqrt(sum(lastloss)/nrow(X)), fitted = fits ) } Run Set starting values. init = rep(0, 3) For any particular data you might have to fiddle with the stepsize, perhaps choosing one based on cross-validation with old data. sgd_result = sgd( init, X = X, y = y, stepsize = .1, stepsizeTau = .5, average = FALSE ) str(sgd_result) List of 5 $ par : num [1:3, 1] 1.024 0.537 0.148 ..- attr(*, &quot;dimnames&quot;)=List of 2 .. ..$ : chr [1:3] &quot;Intercept&quot; &quot;x1&quot; &quot;x2&quot; .. ..$ : NULL $ parvec: num [1:1000, 1:3] -0.06208 -0.00264 0.04781 0.09866 0.08242 ... $ loss : num [1:1000] 0.67 1.261 1.365 2.043 0.215 ... $ RMSE : num 1.01 $ fitted: num [1:1000] 0 -0.0236 -0.0446 -0.2828 0.1634 ... sgd_result$par [,1] Intercept 1.0241049 x1 0.5368198 x2 0.1478470 Comparison We can compare to standard linear regression. # summary(lm(y ~ x1 + x2)) coef1 = coef(lm(y ~ x1 + x2)) rbind( sgd_result = sgd_result$par[, 1], lm = coef1 ) Intercept x1 x2 sgd_result 1.024105 0.5368198 0.1478470 lm 1.029957 0.5177020 0.1631026 Visualize Estimates library(tidyverse) gd = data.frame(sgd_result$parvec) %&gt;% mutate(Iteration = 1:n()) gd = gd %&gt;% pivot_longer(cols = -Iteration, names_to = &#39;Parameter&#39;, values_to = &#39;Value&#39;) %&gt;% mutate(Parameter = factor(Parameter, labels = colnames(X))) ggplot(aes( x = Iteration, y = Value, group = Parameter, color = Parameter ), data = gd) + geom_path() + geom_point(data = filter(gd, Iteration == n), size = 3) + geom_text( aes(label = round(Value, 2)), hjust = -.5, angle = 45, size = 4, data = filter(gd, Iteration == n) ) + theme_minimal() Data Set Shift This data includes a shift of the previous data. set.seed(1234) n2 = 1000 x1.2 = rnorm(n2) x2.2 = rnorm(n2) y2 = -1 + .25*x1.2 - .25*x2.2 + rnorm(n2) X2 = rbind(X, cbind(1, x1.2, x2.2)) coef2 = coef(lm(y2 ~ x1.2 + x2.2)) y2 = c(y, y2) n3 = 1000 x1.3 = rnorm(n3) x2.3 = rnorm(n3) y3 = 1 - .25*x1.3 + .25*x2.3 + rnorm(n3) coef3 = coef(lm(y3 ~ x1.3 + x2.3)) X3 = rbind(X2, cbind(1, x1.3, x2.3)) y3 = c(y2, y3) Run sgd_result2 = sgd( init, X = X3, y = y3, stepsize = 1, stepsizeTau = 0, average = FALSE ) str(sgd_result2) List of 5 $ par : num [1:3, 1] 0.821 -0.223 0.211 ..- attr(*, &quot;dimnames&quot;)=List of 2 .. ..$ : chr [1:3] &quot;Intercept&quot; &quot;x1&quot; &quot;x2&quot; .. ..$ : NULL $ parvec: num [1:3000, 1:3] -1 -0.119 0.624 1.531 1.063 ... $ loss : num [1:3000] 0.67 2.31 3.69 30.99 10.58 ... $ RMSE : num 1.57 $ fitted: num [1:3000] 0 -0.421 -0.797 -4.421 2.952 ... Comparison Compare with lm for each data part. sgd_result2$parvec[c(n, n + n2, n + n2 + n3), ] [,1] [,2] [,3] [1,] 1.0859378 0.5128904 0.1457697 [2,] -0.9246994 0.2945723 -0.2941759 [3,] 0.8213521 -0.2229918 0.2112883 rbind(coef1, coef2, coef3) (Intercept) x1 x2 coef1 1.0299573 0.5177020 0.1631026 coef2 -0.9700427 0.2677020 -0.2868974 coef3 1.0453166 -0.2358521 0.2418489 Visualize Estimates Visualize estimates. gd = data.frame(sgd_result2$parvec) %&gt;% mutate(Iteration = 1:n()) gd = gd %&gt;% pivot_longer(cols = -Iteration, names_to = &#39;Parameter&#39;, values_to = &#39;Value&#39;) %&gt;% mutate(Parameter = factor(Parameter, labels = colnames(X))) ggplot(aes(x = Iteration, y = Value, group = Parameter, color = Parameter ), data = gd) + geom_path() + geom_point(data = filter(gd, Iteration %in% c(n, n + n2, n + n2 + n3)), size = 3) + geom_text( aes(label = round(Value, 2)), hjust = -.5, angle = 45, data = filter(gd, Iteration %in% c(n, n + n2, n + n2 + n3)), size = 4, show.legend = FALSE ) + theme_minimal() Source Base R source code found at https://github.com/m-clark/Miscellaneous-R-Code/blob/master/ModelFitting/stochastic_gradient_descent.R "]]
